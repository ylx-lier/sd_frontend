import os
import sys
import signal
import subprocess
import requests
import torch
import numpy as np
import cv2
import gradio as gr
import warnings
import io
import base64
from PIL import Image
from datetime import datetime
from diffusers import StableDiffusionPipeline, StableDiffusionControlNetPipeline, ControlNetModel
from diffusers import StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler

warnings.filterwarnings("ignore")

# å…¨å±€å˜é‡å­˜å‚¨Gradioåº”ç”¨å®ä¾‹
gradio_app = None
current_port = None

def cleanup_resources():
    """æ¸…ç†èµ„æºå’Œé‡Šæ”¾ç«¯å£"""
    global gradio_app, current_port
    
    print("\nğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
    
    try:
        # å…³é—­Gradioåº”ç”¨
        if gradio_app is not None:
            print("ğŸ“± å…³é—­Gradioåº”ç”¨...")
            try:
                gradio_app.close()
            except:
                pass
            gradio_app = None
        
        # å¼ºåˆ¶é‡Šæ”¾ç«¯å£ï¼ˆWindowsï¼‰
        if current_port and os.name == 'nt':  # Windowsç³»ç»Ÿ
            try:
                print(f"ğŸ”Œ é‡Šæ”¾ç«¯å£ {current_port}...")
                # æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
                result = subprocess.run(
                    f'netstat -ano | findstr :{current_port}',
                    shell=True, capture_output=True, text=True
                )
                
                if result.stdout:
                    lines = result.stdout.strip().split('\n')
                    for line in lines:
                        if 'LISTENING' in line:
                            parts = line.split()
                            if len(parts) >= 5:
                                pid = parts[-1]
                                print(f"ğŸ¯ ç»ˆæ­¢è¿›ç¨‹ PID: {pid}")
                                subprocess.run(f'taskkill /f /pid {pid}', shell=True, capture_output=True)
                                
            except Exception as e:
                print(f"âš ï¸ ç«¯å£é‡Šæ”¾è­¦å‘Š: {e}")
        
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ - æ•è·Ctrl+Cç­‰ä¸­æ–­ä¿¡å·"""
    print(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å· {signum}")
    cleanup_resources()
    print("ğŸ‘‹ åº”ç”¨å·²å®‰å…¨é€€å‡º")
    sys.exit(0)

def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
    try:
        # æ•è·å¸¸è§çš„ä¸­æ–­ä¿¡å·
        signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, signal_handler) # ç»ˆæ­¢ä¿¡å·
        if hasattr(signal, 'SIGBREAK'):
            signal.signal(signal.SIGBREAK, signal_handler) # Windows Ctrl+Break
        print("ğŸ”§ ä¿¡å·å¤„ç†å™¨å·²è®¾ç½®")
    except Exception as e:
        print(f"âš ï¸ ä¿¡å·å¤„ç†å™¨è®¾ç½®å¤±è´¥: {e}")

# è®¾ç½®è‡ªåŠ¨æ¸…ç†
import atexit
atexit.register(cleanup_resources)

# å…¨å±€å˜é‡å­˜å‚¨ç®¡é“
pipe = None
controlnet_pipe = None
img2img_pipe = None
device = "cuda" if torch.cuda.is_available() else "cpu"
current_model = "runwayml/stable-diffusion-v1-5"
current_controlnet = None

# è¿è¡Œæ¨¡å¼é€‰æ‹©
RUN_MODE = "api"  # "local" æˆ– "api"
HF_API_TOKEN = None  # åœ¨è¿™é‡Œè®¾ç½®æ‚¨çš„ Hugging Face API Token

# ä»£ç†è®¾ç½® (ç”¨äºè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜)
PROXY_CONFIG = {
    "enabled": False,
    "http": None,
    "https": None
}

def update_proxy_config(enabled, http_proxy, https_proxy):
    """æ›´æ–°ä»£ç†é…ç½®"""
    global PROXY_CONFIG
    PROXY_CONFIG["enabled"] = enabled
    PROXY_CONFIG["http"] = http_proxy if http_proxy.strip() else None
    PROXY_CONFIG["https"] = https_proxy if https_proxy.strip() else None
    
    if enabled and (PROXY_CONFIG["http"] or PROXY_CONFIG["https"]):
        return f"âœ… ä»£ç†å·²å¯ç”¨: HTTP={PROXY_CONFIG['http'] or 'None'}, HTTPS={PROXY_CONFIG['https'] or 'None'}"
    else:
        return "âŒ ä»£ç†å·²ç¦ç”¨"

def auto_push_to_github():
    """è‡ªåŠ¨æ¨é€åˆ° GitHub"""
    try:
        print("ğŸš€ å¼€å§‹è‡ªåŠ¨æ¨é€åˆ° GitHub...")
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ git ä»“åº“ä¸­
        result = subprocess.run("git status", shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            return "âŒ å½“å‰ç›®å½•ä¸æ˜¯ git ä»“åº“æˆ– git æœªå®‰è£…"
        
        # æ·»åŠ æ‰€æœ‰æ›´æ”¹çš„æ–‡ä»¶
        result = subprocess.run("git add .", shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            return f"âŒ æ·»åŠ æ–‡ä»¶å¤±è´¥: {result.stderr}"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ”¹éœ€è¦æäº¤
        result = subprocess.run("git diff --staged --quiet", shell=True, capture_output=True, text=True)
        if result.returncode == 0:  # å¦‚æœå‘½ä»¤æˆåŠŸï¼Œè¯´æ˜æ²¡æœ‰æ›´æ”¹
            return "âœ… æ²¡æœ‰æ–°çš„æ›´æ”¹éœ€è¦æäº¤"
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # å‡†å¤‡æäº¤ä¿¡æ¯
        commit_message = f"Auto update: {timestamp} - åŠŸèƒ½æ›´æ–°å’Œä¼˜åŒ–"
        
        # æäº¤æ›´æ”¹
        result = subprocess.run(f'git commit -m "{commit_message}"', shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            return f"âŒ æäº¤å¤±è´¥: {result.stderr}"
        
        # æ¨é€åˆ°è¿œç¨‹ä»“åº“
        result = subprocess.run("git push origin main", shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            return f"âŒ æ¨é€å¤±è´¥: {result.stderr}\nğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– GitHub æƒé™"
        
        # è·å–ä»“åº“ URL
        result = subprocess.run("git remote get-url origin", shell=True, capture_output=True, text=True)
        repo_url = result.stdout.strip() if result.returncode == 0 else "æœªçŸ¥"
        
        return f"âœ… æˆåŠŸæ¨é€åˆ° GitHub!\nğŸ”— ä»“åº“: {repo_url}\nâ° æ—¶é—´: {timestamp}"
        
    except Exception as e:
        return f"âŒ æ¨é€è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

# APIæ¨¡å¼ä¸‹çš„æ¨ç†ç«¯ç‚¹ - å®˜æ–¹æ”¯æŒçš„çƒ­é—¨æ¨¡å‹
API_ENDPOINTS = {
    # æœ€æ–°æ¨èæ¨¡å‹ (å®˜æ–¹æ–‡æ¡£æ¨è)
    "black-forest-labs/FLUX.1-dev": "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev",
    "black-forest-labs/FLUX.1-schnell": "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell",
    "stabilityai/stable-diffusion-xl-base-1.0": "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0",
    "stabilityai/stable-diffusion-3.5-large": "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3.5-large",
    "stabilityai/stable-diffusion-3-medium-diffusers": "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3-medium-diffusers",
    "latent-consistency/lcm-lora-sdxl": "https://api-inference.huggingface.co/models/latent-consistency/lcm-lora-sdxl",
    "Kwai-Kolors/Kolors": "https://api-inference.huggingface.co/models/Kwai-Kolors/Kolors",
    
    # ç»å…¸ç¨³å®šçš„APIæ¨¡å‹
    "runwayml/stable-diffusion-v1-5": "https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5",
    "stabilityai/stable-diffusion-2-1": "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-2-1",
    "prompthero/openjourney": "https://api-inference.huggingface.co/models/prompthero/openjourney",
    "dreamlike-art/dreamlike-diffusion-1.0": "https://api-inference.huggingface.co/models/dreamlike-art/dreamlike-diffusion-1.0",
}

# ControlNet API endpoints - æ›´æ–°ä¸ºæœ€æ–°ç‰ˆæœ¬
CONTROLNET_API_ENDPOINTS = {
    "canny": "https://api-inference.huggingface.co/models/lllyasviel/control_v11p_sd15_canny",
    "scribble": "https://api-inference.huggingface.co/models/lllyasviel/control_v11p_sd15_scribble", 
    "depth": "https://api-inference.huggingface.co/models/lllyasviel/control_v11p_sd15_depth",
    "openpose": "https://api-inference.huggingface.co/models/lllyasviel/control_v11p_sd15_openpose",
    "seg": "https://api-inference.huggingface.co/models/lllyasviel/control_v11p_sd15_seg"
}

def validate_api_key(api_token):
    """éªŒè¯API Keyçš„æœ‰æ•ˆæ€§ - æ”¹è¿›ç‰ˆæœ¬"""
    if not api_token.strip():
        return "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„API Token"
    
    token = api_token.strip()
    
    # åŸºæœ¬æ ¼å¼æ£€æŸ¥
    if not token.startswith('hf_'):
        return "âŒ Tokenæ ¼å¼é”™è¯¯ï¼šåº”è¯¥ä»¥ 'hf_' å¼€å¤´"
    
    if len(token) < 30:
        return "âŒ Tokené•¿åº¦è¿‡çŸ­ï¼šè¯·æ£€æŸ¥æ˜¯å¦å®Œæ•´å¤åˆ¶"
    
    try:
        # æ„å»ºä»£ç†é…ç½®
        proxies = None
        if PROXY_CONFIG.get('enabled'):
            proxies = {}
            if PROXY_CONFIG.get('http'):
                proxies['http'] = PROXY_CONFIG['http']
            if PROXY_CONFIG.get('https'):
                proxies['https'] = PROXY_CONFIG['https']
        
        headers = {"Authorization": f"Bearer {token}"}
        
        # æ–¹æ³•1: å°è¯•è®¿é—®ç”¨æˆ·ä¿¡æ¯API (ä½¿ç”¨æ­£ç¡®çš„v2ç«¯ç‚¹)
        try:
            response = requests.get(
                "https://huggingface.co/api/whoami-v2",
                headers=headers,
                timeout=15,
                proxies=proxies
            )
            
            if response.status_code == 200:
                try:
                    user_info = response.json()
                    username = user_info.get('name', 'User')
                    return f"âœ… TokenéªŒè¯æˆåŠŸ - ç”¨æˆ·: {username}"
                except:
                    return f"âœ… TokenéªŒè¯æˆåŠŸ - APIå“åº”æ­£å¸¸"
            elif response.status_code == 401:
                return "âŒ Tokenæ— æ•ˆï¼šè¯·æ£€æŸ¥Tokenæ˜¯å¦æ­£ç¡®æˆ–å·²è¿‡æœŸ"
            elif response.status_code == 403:
                return "âš ï¸ Tokenæƒé™å—é™ï¼Œä½†å¯èƒ½å¯ç”¨äºåŸºç¡€APIè°ƒç”¨"
            else:
                # å¦‚æœwhoamiå¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–éªŒè¯æ–¹æ³•
                pass
                
        except requests.exceptions.RequestException:
            # whoami APIå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            pass
        
        # æ–¹æ³•2: å°è¯•è®¿é—®æ¨¡å‹åˆ—è¡¨APIï¼ˆæ›´å®½æ¾çš„éªŒè¯ï¼‰
        try:
            response = requests.get(
                "https://huggingface.co/api/models",
                headers=headers,
                timeout=15,
                proxies=proxies,
                params={"limit": 1}  # åªè¯·æ±‚1ä¸ªæ¨¡å‹ï¼Œå‡å°‘æµé‡
            )
            
            if response.status_code == 200:
                return f"âœ… TokenåŸºæœ¬æœ‰æ•ˆ - å¯è®¿é—®æ¨¡å‹API"
            elif response.status_code == 401:
                return "âŒ Tokenæ— æ•ˆæˆ–å·²è¿‡æœŸ"
            elif response.status_code == 403:
                return "âš ï¸ Tokenæƒé™ä¸è¶³ï¼Œä½†æ ¼å¼æ­£ç¡®"
            else:
                return f"âš ï¸ APIè¿”å›çŠ¶æ€ {response.status_code}ï¼Œè¯·æ£€æŸ¥Tokenæƒé™"
                
        except requests.exceptions.RequestException:
            pass
        
        # æ–¹æ³•3: æœ€åå°è¯•ç®€å•çš„æ¨ç†APIæ£€æŸ¥ï¼ˆHEADè¯·æ±‚ï¼‰
        try:
            test_endpoint = "https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5"
            response = requests.head(
                test_endpoint,
                headers=headers,
                timeout=10,
                proxies=proxies
            )
            
            if response.status_code in [200, 503]:  # 503è¡¨ç¤ºæ¨¡å‹åœ¨åŠ è½½
                return f"âœ… Tokenå¯ç”¨äºæ¨ç†API"
            elif response.status_code == 401:
                return "âŒ Tokenæ— æ•ˆï¼Œæ— æ³•è®¿é—®æ¨ç†API"
            elif response.status_code == 403:
                return "âŒ Tokenæƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®æ¨ç†API"
            else:
                return f"âš ï¸ æ¨ç†APIè¿”å›çŠ¶æ€ {response.status_code}ï¼ŒTokenå¯èƒ½æœ‰æ•ˆ"
                
        except requests.exceptions.Timeout:
            return f"âš ï¸ ç½‘ç»œè¶…æ—¶ï¼ŒTokenæ ¼å¼æ­£ç¡®ä½†æ— æ³•éªŒè¯è¿æ¥"
        except requests.exceptions.ConnectionError:
            return f"âš ï¸ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ–ä»£ç†é…ç½®"
            
    except Exception as e:
        return f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)[:50]}..."
    
    # å¦‚æœæ‰€æœ‰APIè°ƒç”¨éƒ½å¤±è´¥ï¼Œä½†Tokenæ ¼å¼æ­£ç¡®
    return f"âš ï¸ æ— æ³•éªŒè¯Tokenæœ‰æ•ˆæ€§ï¼Œä½†æ ¼å¼æ­£ç¡®ã€‚å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIæœåŠ¡å¼‚å¸¸"

def check_model_api_support(model_id, run_mode):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒAPIæ¨¡å¼"""
    if run_mode != "api":
        return f"âœ… æœ¬åœ°æ¨¡å¼ - æ”¯æŒæ‰€æœ‰æ¨¡å‹"
    
    if model_id in API_ENDPOINTS:
        return f"âœ… APIæ¨¡å¼æ”¯æŒ - {MODELS.get(model_id, model_id)}"
    else:
        available_models = ", ".join([MODELS.get(m, m) for m in API_ENDPOINTS.keys()])
        return f"âŒ APIæ¨¡å¼ä¸æ”¯æŒæ­¤æ¨¡å‹\nğŸ’¡ æ”¯æŒçš„æ¨¡å‹: {available_models}"

def test_model_api_connection(model_id, api_token):
    """æµ‹è¯•æ¨¡å‹APIè¿æ¥ - æ”¹è¿›ç‰ˆæœ¬"""
    if not api_token.strip():
        return "âš ï¸ è¯·å…ˆè¾“å…¥æœ‰æ•ˆçš„API Token"
    
    if model_id not in API_ENDPOINTS:
        return f"âŒ æ¨¡å‹ {model_id} ä¸æ”¯æŒAPIæ¨¡å¼"
    
    try:
        endpoint = API_ENDPOINTS[model_id]
        headers = {"Authorization": f"Bearer {api_token.strip()}"}
        
        # æ„å»ºä»£ç†é…ç½®
        proxies = None
        if PROXY_CONFIG.get('enabled'):
            proxies = {}
            if PROXY_CONFIG.get('http'):
                proxies['http'] = PROXY_CONFIG['http']
            if PROXY_CONFIG.get('https'):
                proxies['https'] = PROXY_CONFIG['https']
        
        # ä½¿ç”¨HEADè¯·æ±‚æ£€æŸ¥APIå¯è®¿é—®æ€§ï¼ˆä¸å®é™…ç”Ÿæˆå›¾ç‰‡ï¼‰
        response = requests.head(
            endpoint,
            headers=headers,
            timeout=10,
            proxies=proxies
        )
        
        model_name = MODELS.get(model_id, model_id)
        
        if response.status_code == 200:
            return f"âœ… æ¨¡å‹APIè¿æ¥æˆåŠŸ - {model_name} å¯ç”¨"
        elif response.status_code == 503:
            return f"âš ï¸ æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­ - {model_name} (è¯·ç¨åé‡è¯•)"
        elif response.status_code == 401:
            return "âŒ API Tokenæ— æ•ˆæˆ–æ— æƒé™è®¿é—®æ­¤æ¨¡å‹"
        elif response.status_code == 403:
            return "âŒ Tokenæƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®æ¨ç†API"
        elif response.status_code == 404:
            return f"âŒ æ¨¡å‹ç«¯ç‚¹ä¸å­˜åœ¨ - {model_name}"
        elif response.status_code == 429:
            return f"âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ - {model_name} (Tokenæœ‰æ•ˆ)"
        else:
            return f"âš ï¸ APIè¿”å›çŠ¶æ€ç  {response.status_code} - è¿æ¥å¯èƒ½æœ‰é—®é¢˜"
    
    except requests.exceptions.Timeout:
        return f"âŒ è¿æ¥è¶…æ—¶ - è¯·æ£€æŸ¥ç½‘ç»œæˆ–å¯ç”¨ä»£ç†"
    except requests.exceptions.ConnectionError:
        return f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥ - è¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ–ä»£ç†é…ç½®"
    except Exception as e:
        return f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)[:50]}..."

def test_controlnet_api_connection(control_type, api_token):
    """æµ‹è¯•ControlNet APIè¿æ¥"""
    if not api_token.strip():
        return "âš ï¸ è¯·å…ˆè¾“å…¥æœ‰æ•ˆçš„API Token"
    
    if control_type not in CONTROLNET_API_ENDPOINTS:
        return f"âŒ ControlNetç±»å‹ {control_type} ä¸æ”¯æŒAPIæ¨¡å¼"
    
    try:
        endpoint = CONTROLNET_API_ENDPOINTS[control_type]
        headers = {"Authorization": f"Bearer {api_token.strip()}"}
        
        # æ„å»ºä»£ç†é…ç½®
        proxies = None
        if PROXY_CONFIG.get('enabled'):
            proxies = {}
            if PROXY_CONFIG.get('http'):
                proxies['http'] = PROXY_CONFIG['http']
            if PROXY_CONFIG.get('https'):
                proxies['https'] = PROXY_CONFIG['https']
        
        # ä½¿ç”¨HEADè¯·æ±‚æ£€æŸ¥APIå¯è®¿é—®æ€§
        response = requests.head(
            endpoint,
            headers=headers,
            timeout=10,
            proxies=proxies
        )
        
        control_name = CONTROLNET_TYPES[control_type]['name']
        
        if response.status_code == 200:
            return f"âœ… ControlNet APIè¿æ¥æˆåŠŸ - {control_name} å¯ç”¨"
        elif response.status_code == 503:
            return f"âš ï¸ ControlNetæ¨¡å‹æ­£åœ¨åŠ è½½ - {control_name} (è¯·ç¨ç­‰1-2åˆ†é’Ÿ)"
        elif response.status_code == 401:
            return "âŒ API Tokenæ— æ•ˆæˆ–æ— æƒé™è®¿é—®ControlNetæ¨¡å‹"
        elif response.status_code == 403:
            return "âŒ Tokenæƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®ControlNetæ¨ç†API"
        elif response.status_code == 404:
            return f"âŒ ControlNetç«¯ç‚¹ä¸å­˜åœ¨ - {control_name}"
        elif response.status_code == 429:
            return f"âš ï¸ APIè°ƒç”¨é¢‘ç‡é™åˆ¶ - {control_name} (Tokenæœ‰æ•ˆï¼Œè¯·ç¨åé‡è¯•)"
        else:
            return f"âš ï¸ æœªçŸ¥çŠ¶æ€ ({response.status_code}) - {control_name}"
    
    except requests.exceptions.Timeout:
        return f"âŒ è¿æ¥è¶…æ—¶ - è¯·æ£€æŸ¥ç½‘ç»œæˆ–å¯ç”¨ä»£ç†"
    except requests.exceptions.ConnectionError:
        return f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥ - è¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ–ä»£ç†é…ç½®"
    except Exception as e:
        return f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)[:50]}..."

def test_img2img_api_connection(api_token):
    """img2img APIæŠ€æœ¯è¯´æ˜ - è§£é‡Šä¸ºä»€ä¹ˆAPIæ¨¡å¼ä¸é€‚åˆimg2img"""
    
    return """ğŸ”¬ img2imgæŠ€æœ¯åŸç†åˆ†æ

ğŸ“‹ **æŠ€æœ¯æ ˆè¦æ±‚**ï¼š
âœ… VAEç¼–ç å™¨ - å›¾åƒâ†’æ½œåœ¨ç©ºé—´è½¬æ¢
âœ… å™ªå£°è°ƒåˆ¶å™¨ - æ ¹æ®strengthå‚æ•°è°ƒåˆ¶
âœ… UNeté‡‡æ ·å™¨ - æ½œåœ¨ç©ºé—´å»å™ªè¿‡ç¨‹  
âœ… VAEè§£ç å™¨ - æ½œåœ¨ç©ºé—´â†’å›¾åƒè½¬æ¢
âœ… è°ƒåº¦å™¨ - æ§åˆ¶é‡‡æ ·æ­¥éª¤

ğŸŒ **APIæ¨¡å¼ç°çŠ¶**ï¼š
âŒ å…¬å…±APIé€šå¸¸åªæä¾›text-to-imageæ¥å£
âŒ ç¼ºå°‘ç‹¬ç«‹çš„VAEç¼–ç å™¨è®¿é—®
âŒ æ— æ³•è¿›è¡Œæ½œåœ¨ç©ºé—´æ“ä½œ
âŒ å¤æ‚æµç¨‹ä¸é€‚åˆHTTP APIå°è£…

ğŸ’¡ **ComfyUIå¯¹æ¯”**ï¼š
ComfyUIé€šè¿‡èŠ‚ç‚¹åŒ–è®¾è®¡ï¼Œæä¾›å®Œæ•´çš„VAEç¼–ç å™¨èŠ‚ç‚¹ï¼Œ
è¿™æ­£æ˜¯é«˜è´¨é‡img2imgçš„å…³é”®ã€‚æ¯ä¸ªæ­¥éª¤éƒ½å¯ä»¥ç‹¬ç«‹é…ç½®ã€‚

ğŸ¯ **æ¨èè§£å†³æ–¹æ¡ˆ**ï¼š

1. ğŸ  **æœ¬åœ°æ¨¡å¼** (æœ€ä½³é€‰æ‹©)
   â€¢ å®Œæ•´img2imgç®¡é“æ”¯æŒ
   â€¢ åŒ…å«ä¸“ç”¨VAEç¼–ç å™¨/è§£ç å™¨
   â€¢ å¯ç²¾ç¡®æ§åˆ¶strengthç­‰å‚æ•°

2. ğŸ–¼ï¸ **ControlNetæ¨¡å¼** (APIå…¼å®¹)
   â€¢ Cannyè¾¹ç¼˜æ£€æµ‹ + æ–‡ç”Ÿå›¾
   â€¢ ä¿æŒåŸå›¾ç»“æ„ï¼Œæ”¹å˜é£æ ¼  
   â€¢ APIæ¨¡å¼å®Œå…¨æ”¯æŒ

3. ğŸ“Š **Inpaintingæ¨¡å¼** (å±€éƒ¨ç¼–è¾‘)
   â€¢ é’ˆå¯¹ç‰¹å®šåŒºåŸŸä¿®æ”¹
   â€¢ æŸäº›APIæœåŠ¡æ”¯æŒ

ï¿½ **æŠ€æœ¯ç»“è®º**ï¼š
img2imgæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªéœ€è¦å®Œæ•´æ¨¡å‹ç®¡é“çš„å¤æ‚æµç¨‹ï¼Œ
æ›´é€‚åˆæœ¬åœ°è®¡ç®—è€ŒéAPIè°ƒç”¨ã€‚"""

# ControlNet ç±»å‹é€‰é¡¹ - æ›´æ–°ä¸ºæœ€æ–°ç‰ˆæœ¬
CONTROLNET_TYPES = {
    "canny": {
        "name": "Cannyè¾¹ç¼˜æ£€æµ‹",
        "model_id": "lllyasviel/control_v11p_sd15_canny",
        "description": "æ£€æµ‹å›¾åƒè¾¹ç¼˜è½®å»“ï¼Œä¿æŒç‰©ä½“å½¢çŠ¶"
    },
    "scribble": {
        "name": "Scribbleæ¶‚é¸¦æ§åˆ¶",
        "model_id": "lllyasviel/control_v11p_sd15_scribble", 
        "description": "åŸºäºæ‰‹ç»˜æ¶‚é¸¦æˆ–ç®€ç¬”ç”»ç”Ÿæˆå›¾åƒ"
    },
    "depth": {
        "name": "Depthæ·±åº¦æ§åˆ¶",
        "model_id": "lllyasviel/control_v11p_sd15_depth",
        "description": "åŸºäºæ·±åº¦å›¾æ§åˆ¶ç©ºé—´ç»“æ„å’Œå±‚æ¬¡"
    },
    "openpose": {
        "name": "OpenPoseå§¿æ€æ§åˆ¶",
        "model_id": "lllyasviel/control_v11p_sd15_openpose",
        "description": "åŸºäºäººä½“å§¿æ€éª¨æ¶æ§åˆ¶äººç‰©å§¿åŠ¿"
    },
    "seg": {
        "name": "Segmentationåˆ†å‰²æ§åˆ¶",
        "model_id": "lllyasviel/control_v11p_sd15_seg",
        "description": "åŸºäºè¯­ä¹‰åˆ†å‰²å›¾æ§åˆ¶ç‰©ä½“åˆ†å¸ƒ"
    }
}

# é¢„å®šä¹‰æ¨¡å‹åˆ—è¡¨ (åˆ†ä¸ºAPIæ”¯æŒå’Œä»…æœ¬åœ°æ”¯æŒ)
API_SUPPORTED_MODELS = {
    # æœ€æ–°æ¨èæ¨¡å‹ (å®˜æ–¹æ–‡æ¡£æ¨èï¼Œæ€§èƒ½ä¼˜å¼‚)
    "black-forest-labs/FLUX.1-dev": "FLUX.1 Dev (æœ€å¼ºå¤§çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ¨è)",
    "black-forest-labs/FLUX.1-schnell": "FLUX.1 Schnell (å¿«é€Ÿç”Ÿæˆï¼Œé«˜è´¨é‡)",
    "stabilityai/stable-diffusion-xl-base-1.0": "SDXL Base 1.0 (é«˜åˆ†è¾¨ç‡ï¼Œç»å…¸é€‰æ‹©)",
    "stabilityai/stable-diffusion-3.5-large": "SD 3.5 Large (æœ€æ–°ç‰ˆæœ¬)",
    "stabilityai/stable-diffusion-3-medium-diffusers": "SD 3 Medium (å¼ºå¤§çš„æ–‡ç”Ÿå›¾)",
    "latent-consistency/lcm-lora-sdxl": "LCM-LoRA SDXL (å¿«é€Ÿä¸”å¼ºå¤§)",
    "Kwai-Kolors/Kolors": "Kolors (é€¼çœŸå›¾åƒç”Ÿæˆ)",
    
    # ç»å…¸ç¨³å®šçš„APIæ¨¡å‹
    "runwayml/stable-diffusion-v1-5": "Stable Diffusion v1.5 (ç»å…¸åŸºç¡€æ¨¡å‹)",
    "stabilityai/stable-diffusion-2-1": "Stable Diffusion v2.1 (æ›´é«˜è´¨é‡)",
    "prompthero/openjourney": "OpenJourney (å¤šæ ·åŒ–è‰ºæœ¯é£æ ¼)",
    "dreamlike-art/dreamlike-diffusion-1.0": "Dreamlike Diffusion (æ¢¦å¹»è‰ºæœ¯é£æ ¼)",
}

# ä»…æœ¬åœ°æ¨¡å¼æ”¯æŒçš„æ¨¡å‹
LOCAL_ONLY_MODELS = {
    "wavymulder/Analog-Diffusion": "Analog Diffusion (èƒ¶ç‰‡é£æ ¼)",
    "22h/vintedois-diffusion-v0-1": "VintedoisDiffusion (å¤å¤é£æ ¼)",
    "nitrosocke/Arcane-Diffusion": "Arcane Diffusion (åŠ¨ç”»é£æ ¼)",
    "hakurei/waifu-diffusion": "Waifu Diffusion (åŠ¨æ¼«é£æ ¼)"
}

# æ ¹æ®è¿è¡Œæ¨¡å¼åŠ¨æ€è·å–å¯ç”¨æ¨¡å‹
def get_available_models(run_mode):
    if run_mode == "api":
        return API_SUPPORTED_MODELS
    else:
        # æœ¬åœ°æ¨¡å¼æ”¯æŒæ‰€æœ‰æ¨¡å‹
        return {**API_SUPPORTED_MODELS, **LOCAL_ONLY_MODELS}

# å…¼å®¹æ€§ï¼šä¿æŒåŸæœ‰MODELSå˜é‡
MODELS = {**API_SUPPORTED_MODELS, **LOCAL_ONLY_MODELS}

# Prompt è¾…åŠ©è¯æ¡
PROMPT_CATEGORIES = {
    "è´¨é‡å¢å¼º": [
        "masterpiece", "best quality", "ultra detailed", "extremely detailed", 
        "high resolution", "8k", "4k", "highly detailed", "sharp focus",
        "professional photography", "award winning", "cinematic lighting"
    ],
    "è‰ºæœ¯é£æ ¼": [
        "oil painting", "watercolor", "digital art", "concept art", "illustration",
        "anime style", "cartoon style", "realistic", "photorealistic", "hyperrealistic",
        "art nouveau", "baroque", "impressionist", "surreal", "abstract"
    ],
    "å…‰ç…§æ•ˆæœ": [
        "soft lighting", "dramatic lighting", "cinematic lighting", "golden hour",
        "studio lighting", "natural lighting", "ambient lighting", "rim lighting",
        "volumetric lighting", "god rays", "neon lighting", "sunset", "sunrise"
    ],
    "æ„å›¾è§†è§’": [
        "close-up", "portrait", "full body", "wide shot", "aerial view",
        "bird's eye view", "low angle", "high angle", "profile view",
        "three-quarter view", "dynamic pose", "action shot"
    ],
    "æƒ…ç»ªæ°›å›´": [
        "peaceful", "dramatic", "mysterious", "romantic", "epic", "serene",
        "energetic", "melancholic", "cheerful", "dark", "bright", "cozy",
        "majestic", "elegant", "powerful", "gentle"
    ],
    "ç¯å¢ƒåœºæ™¯": [
        "forest", "mountain", "ocean", "city", "countryside", "desert",
        "fantasy world", "sci-fi", "medieval", "modern", "futuristic",
        "indoor", "outdoor", "studio", "landscape", "urban", "nature"
    ],
    "è‰²å½©é£æ ¼": [
        "vibrant colors", "muted colors", "monochrome", "black and white",
        "warm colors", "cool colors", "pastel colors", "neon colors",
        "earth tones", "jewel tones", "vintage colors", "saturated"
    ]
}

# è´Ÿé¢æç¤ºè¯è¾…åŠ©è¯æ¡
NEGATIVE_PROMPT_CATEGORIES = {
    "ç”»è´¨é—®é¢˜": [
        "blurry", "low quality", "bad quality", "worst quality", "poor quality",
        "pixelated", "jpeg artifacts", "compression artifacts", "distorted",
        "low resolution", "grainy", "noisy", "oversaturated", "undersaturated"
    ],
    "è§£å‰–é”™è¯¯": [
        "bad anatomy", "bad hands", "bad fingers", "extra fingers", "missing fingers",
        "extra limbs", "missing limbs", "deformed", "mutated", "disfigured",
        "malformed", "extra arms", "extra legs", "fused fingers", "too many fingers"
    ],
    "é¢éƒ¨é—®é¢˜": [
        "bad face", "ugly face", "distorted face", "asymmetrical face",
        "bad eyes", "cross-eyed", "extra eyes", "missing eyes", "bad mouth",
        "bad teeth", "crooked teeth", "bad nose", "asymmetrical features"
    ],
    "è‰ºæœ¯é£æ ¼": [
        "cartoon", "anime", "manga", "3d render", "painting", "sketch",
        "watercolor", "oil painting", "digital art", "illustration",
        "abstract", "surreal", "unrealistic", "stylized"
    ],
    "æŠ€æœ¯é—®é¢˜": [
        "watermark", "signature", "text", "logo", "copyright", "username",
        "frame", "border", "cropped", "cut off", "out of frame",
        "duplicate", "error", "glitch", "artifact"
    ],
    "å…‰ç…§é—®é¢˜": [
        "bad lighting", "harsh lighting", "overexposed", "underexposed",
        "too dark", "too bright", "uneven lighting", "poor contrast",
        "washed out", "flat lighting", "artificial lighting"
    ],
    "æ„å›¾é—®é¢˜": [
        "bad composition", "off-center", "tilted", "crooked", "unbalanced",
        "cluttered", "messy", "chaotic", "poor framing", "bad angle",
        "awkward pose", "stiff pose", "unnatural pose"
    ]
}

def load_models(run_mode, selected_model, controlnet_type="canny", api_token=""):
    """åŠ è½½æ¨¡å‹ç®¡é“ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒAPIæ¨¡å‹æ£€æµ‹"""
    global pipe, controlnet_pipe, img2img_pipe, current_model, current_controlnet, RUN_MODE, HF_API_TOKEN
    
    if not selected_model:
        return "âŒ è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹"
    
    # æ›´æ–°å…¨å±€é…ç½®
    RUN_MODE = run_mode
    current_model = selected_model
    if api_token.strip():
        HF_API_TOKEN = api_token.strip()
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    available_models = get_available_models(run_mode)
    model_name = available_models.get(selected_model, selected_model)
    
    if run_mode == "api":
        # APIæ¨¡å¼ - æ£€æŸ¥æ¨¡å‹æ”¯æŒ
        if selected_model not in API_ENDPOINTS:
            supported_models = list(API_SUPPORTED_MODELS.keys())
            recommended = supported_models[:3]  # æ¨èå‰3ä¸ª
            
            return f"âŒ æ¨¡å‹ {model_name} ä¸æ”¯æŒAPIæ¨¡å¼\n\nï¿½ æ¨èæ”¯æŒAPIçš„æ¨¡å‹:\n" + \
                   "\n".join([f"â€¢ {API_SUPPORTED_MODELS[m]}" for m in recommended]) + \
                   f"\n\nğŸ’¡ å…±æœ‰ {len(supported_models)} ä¸ªæ¨¡å‹æ”¯æŒAPIæ¨¡å¼ï¼Œè¯·åœ¨ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©"
        
        # æ£€æŸ¥Tokenæœ‰æ•ˆæ€§ï¼ˆå¦‚æœæä¾›ï¼‰
        token_status = ""
        if api_token.strip():
            # å¯ä»¥åœ¨è¿™é‡Œè°ƒç”¨TokenéªŒè¯å‡½æ•°
            token_status = "\nğŸ”‘ ä½¿ç”¨è®¤è¯Token"
        
        # æ¨¡æ‹ŸåŠ è½½æˆåŠŸ
        pipe = "api_mode"
        img2img_pipe = "api_mode" 
        controlnet_pipe = "api_mode"
        current_controlnet = controlnet_type
        
        # åˆ¤æ–­æ¨¡å‹ç±»å‹å¹¶ç»™å‡ºç›¸åº”æç¤º
        if selected_model.startswith("black-forest-labs/FLUX"):
            quality_tip = "\nâš¡ FLUXç³»åˆ— - æœ€æ–°ä¸€ä»£æ¨¡å‹ï¼Œå›¾åƒè´¨é‡æé«˜"
        elif selected_model.startswith("stabilityai/stable-diffusion-xl"):
            quality_tip = "\nï¿½ SDXLç³»åˆ— - é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼Œç»å…¸é€‰æ‹©"
        elif selected_model.startswith("stabilityai/stable-diffusion-3"):
            quality_tip = "\nğŸš€ SD3ç³»åˆ— - æœ€æ–°æŠ€æœ¯ï¼Œæ–‡æœ¬ç†è§£èƒ½åŠ›å¼º"
        else:
            quality_tip = "\nğŸ“ ç»å…¸æ¨¡å‹ - ç¨³å®šå¯é "
        
        return f"âœ… APIæ¨¡å¼é…ç½®æˆåŠŸï¼\nğŸ“¦ å½“å‰æ¨¡å‹: {model_name}\nğŸ¯ æ¨¡å‹ID: {selected_model}\nğŸ® ControlNet: {CONTROLNET_TYPES[controlnet_type]['name']}{quality_tip}{token_status}\nğŸ’¾ å­˜å‚¨ç©ºé—´å ç”¨: 0 GB\n\nğŸ’¡ APIæ¨¡å¼æ— éœ€ä¸‹è½½æ¨¡å‹ï¼Œç”Ÿæˆå›¾ç‰‡é€šè¿‡äº‘ç«¯æ¨ç†"
    
    else:
        # æœ¬åœ°æ¨¡å¼ - ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
        try:
            # åŸºç¡€æ–‡ç”Ÿå›¾ç®¡é“
            pipe = StableDiffusionPipeline.from_pretrained(
                selected_model,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                safety_checker=None,
                requires_safety_checker=False
            )
            pipe = pipe.to(device)
            pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
            
            # ä¼ ç»Ÿå›¾ç”Ÿå›¾ç®¡é“
            img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
                selected_model,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                safety_checker=None,
                requires_safety_checker=False
            )
            img2img_pipe = img2img_pipe.to(device)
            img2img_pipe.scheduler = DPMSolverMultistepScheduler.from_config(img2img_pipe.scheduler.config)
            
            # ControlNet ç®¡é“
            try:
                current_controlnet = controlnet_type
                controlnet_info = CONTROLNET_TYPES[controlnet_type]
                
                controlnet = ControlNetModel.from_pretrained(
                    controlnet_info["model_id"],
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32
                )
                controlnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(
                    selected_model,
                    controlnet=controlnet,
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                    safety_checker=None,
                    requires_safety_checker=False
                )
                controlnet_pipe = controlnet_pipe.to(device)
                controlnet_pipe.scheduler = DPMSolverMultistepScheduler.from_config(controlnet_pipe.scheduler.config)
                return f"âœ… æœ¬åœ°æ¨¡å¼æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼\nğŸ“¦ å½“å‰æ¨¡å‹: {model_name}\nğŸ¯ æ¨¡å‹ID: {selected_model}\nğŸ® ControlNet: {controlnet_info['name']}\nğŸ’¾ é¢„è®¡å­˜å‚¨å ç”¨: ~6-10 GB"
            except Exception as controlnet_error:
                return f"âœ… æœ¬åœ°æ¨¡å¼åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼\nğŸ“¦ å½“å‰æ¨¡å‹: {model_name}\nğŸ¯ æ¨¡å‹ID: {selected_model}\nâš ï¸ ControlNetåŠ è½½å¤±è´¥: {str(controlnet_error)}\nğŸ’¡ æ–‡ç”Ÿå›¾å’Œä¼ ç»Ÿå›¾ç”Ÿå›¾åŠŸèƒ½å¯æ­£å¸¸ä½¿ç”¨\nğŸ’¾ é¢„è®¡å­˜å‚¨å ç”¨: ~4-7 GB"
            
        except Exception as e:
            return f"âŒ æœ¬åœ°æ¨¡å¼åŠ è½½å¤±è´¥: {str(e)}\nğŸ’¡ å»ºè®®å°è¯•APIæ¨¡å¼ä»¥é¿å…å­˜å‚¨ç©ºé—´é—®é¢˜"

def generate_image(prompt, negative_prompt, num_steps, guidance_scale, width, height, seed):
    """åŸºç¡€æ–‡ç”Ÿå›¾åŠŸèƒ½"""
    global pipe, current_model, RUN_MODE
    
    if pipe is None:
        return None, "Please load the model first"
    
    if RUN_MODE == "api":
        # APIæ¨¡å¼
        try:
            image, status = generate_image_api(prompt, negative_prompt, current_model)
            return image, status
        except Exception as e:
            return None, f"âŒ APIç”Ÿæˆå¤±è´¥: {str(e)}"
    
    else:
        # æœ¬åœ°æ¨¡å¼
        try:
            # è®¾ç½®éšæœºç§å­
            if seed != -1:
                generator = torch.Generator(device=device).manual_seed(seed)
            else:
                generator = None
                
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device):
                result = pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    num_inference_steps=num_steps,
                    guidance_scale=guidance_scale,
                    width=width,
                    height=height,
                    generator=generator
                )
            
            image = result.images[0]
            return image, "âœ… æœ¬åœ°å›¾åƒç”ŸæˆæˆåŠŸï¼"
            
        except Exception as e:
            return None, f"âŒ æœ¬åœ°ç”Ÿæˆå¤±è´¥: {str(e)}"

def preprocess_canny(image, low_threshold=100, high_threshold=200):
    """é¢„å¤„ç†å›¾åƒä¸ºCannyè¾¹ç¼˜"""
    image = np.array(image)
    canny = cv2.Canny(image, low_threshold, high_threshold)
    canny_image = canny[:, :, None]
    canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)
    return Image.fromarray(canny_image)

def preprocess_scribble(image):
    """é¢„å¤„ç†å›¾åƒä¸ºæ¶‚é¸¦é£æ ¼ï¼ˆç®€åŒ–è¾¹ç¼˜ï¼‰"""
    image = np.array(image)
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    # ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ä½†å‚æ•°æ›´å®½æ¾ï¼Œæ¨¡æ‹Ÿæ¶‚é¸¦æ•ˆæœ
    edges = cv2.Canny(gray, 50, 150)
    # è†¨èƒ€æ“ä½œä½¿çº¿æ¡æ›´ç²—ï¼Œæ›´åƒæ¶‚é¸¦
    kernel = np.ones((3,3), np.uint8)
    edges = cv2.dilate(edges, kernel, iterations=1)
    # è½¬æ¢å›RGB
    scribble_image = edges[:, :, None]
    scribble_image = np.concatenate([scribble_image, scribble_image, scribble_image], axis=2)
    return Image.fromarray(scribble_image)

def preprocess_depth(image):
    """é¢„å¤„ç†å›¾åƒä¸ºæ·±åº¦å›¾ï¼ˆä½¿ç”¨ç®€å•çš„æ·±åº¦ä¼°è®¡ï¼‰"""
    image = np.array(image)
    # è½¬æ¢ä¸ºç°åº¦å›¾ä½œä¸ºç®€å•çš„æ·±åº¦ä¼°è®¡
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    # åº”ç”¨é«˜æ–¯æ¨¡ç³Šæ¥æ¨¡æ‹Ÿæ·±åº¦æ„Ÿ
    depth = cv2.GaussianBlur(gray, (5, 5), 0)
    # å¢å¼ºå¯¹æ¯”åº¦
    depth = cv2.equalizeHist(depth)
    # è½¬æ¢å›RGB
    depth_image = depth[:, :, None]
    depth_image = np.concatenate([depth_image, depth_image, depth_image], axis=2)
    return Image.fromarray(depth_image)

def preprocess_control_image(image, control_type):
    """æ ¹æ®æ§åˆ¶ç±»å‹é¢„å¤„ç†å›¾åƒ"""
    if control_type == "canny":
        return preprocess_canny(image)
    elif control_type == "scribble":
        return preprocess_scribble(image)
    elif control_type == "depth":
        return preprocess_depth(image)
    else:
        return preprocess_canny(image)  # é»˜è®¤ä½¿ç”¨canny

def generate_controlnet_image(prompt, negative_prompt, control_image, control_type, num_steps, guidance_scale, controlnet_conditioning_scale, width, height, seed):
    """ControlNetå›¾åƒå¼•å¯¼ç”Ÿæˆ"""
    global controlnet_pipe, current_controlnet, RUN_MODE
    
    if controlnet_pipe is None:
        return None, None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    if control_image is None:
        return None, None, "âŒ è¯·ä¸Šä¼ æ§åˆ¶å›¾åƒ"
    
    # æ£€æŸ¥å½“å‰åŠ è½½çš„ControlNetç±»å‹æ˜¯å¦åŒ¹é…
    if current_controlnet != control_type:
        return None, None, f"âŒ å½“å‰åŠ è½½çš„æ˜¯ {CONTROLNET_TYPES[current_controlnet]['name']}ï¼Œè¯·é‡æ–°åŠ è½½æ¨¡å‹é€‰æ‹© {CONTROLNET_TYPES[control_type]['name']}"
    
    # é¢„å¤„ç†æ§åˆ¶å›¾åƒ
    processed_image = preprocess_control_image(control_image, control_type)
    
    if RUN_MODE == "api":
        # APIæ¨¡å¼
        try:
            image, status = generate_controlnet_image_api(prompt, negative_prompt, processed_image, control_type)
            return image, processed_image, status
        except Exception as e:
            return None, processed_image, f"âŒ APIç”Ÿæˆå¤±è´¥: {str(e)}"
    
    else:
        # æœ¬åœ°æ¨¡å¼
        try:
            # è®¾ç½®éšæœºç§å­
            if seed != -1:
                generator = torch.Generator(device=device).manual_seed(seed)
            else:
                generator = None
                
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device):
                result = controlnet_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    image=processed_image,
                    num_inference_steps=num_steps,
                    guidance_scale=guidance_scale,
                    controlnet_conditioning_scale=controlnet_conditioning_scale,
                    width=width,
                    height=height,
                    generator=generator
                )
            
            image = result.images[0]
            control_type_name = CONTROLNET_TYPES[control_type]['name']
            return image, processed_image, f"âœ… {control_type_name}å›¾åƒç”ŸæˆæˆåŠŸï¼"
            
        except Exception as e:
            return None, processed_image, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"

def generate_img2img(prompt, negative_prompt, input_image, strength, num_steps, guidance_scale, width, height, seed):
    """ä¼ ç»Ÿå›¾ç”Ÿå›¾åŠŸèƒ½ - æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒ…å«æ›´å¥½çš„APIå¤„ç†å’Œç”¨æˆ·æŒ‡å¯¼"""
    global img2img_pipe, RUN_MODE
    
    if img2img_pipe is None:
        return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    if input_image is None:
        return None, "âŒ è¯·ä¸Šä¼ è¾“å…¥å›¾åƒ"
    
    # éªŒè¯å‚æ•°
    if not prompt or not prompt.strip():
        return None, "âŒ è¯·è¾“å…¥æè¿°æç¤ºè¯"
    
    if strength < 0 or strength > 1:
        return None, "âŒ å˜æ¢å¼ºåº¦åº”åœ¨0-1ä¹‹é—´"
    
    # è°ƒæ•´å›¾åƒå¤§å°
    try:
        input_image = input_image.resize((width, height))
    except Exception as e:
        return None, f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {str(e)}"
    
    if RUN_MODE == "api":
        # APIæ¨¡å¼ - åŒ…å«è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œç”¨æˆ·æŒ‡å¯¼
        try:
            print(f"ğŸŒ APIæ¨¡å¼: å°è¯•img2imgç”Ÿæˆ...")
            image, status = generate_img2img_api(prompt, negative_prompt, input_image, strength)
            
            if image is not None:
                return image, status
            else:
                # APIå¤±è´¥æ—¶ï¼Œæä¾›è¯¦ç»†çš„æŒ‡å¯¼ä¿¡æ¯
                fallback_message = f"""ğŸ”„ img2img APIæš‚ä¸å¯ç”¨ï¼Œå»ºè®®å°è¯•ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆ:

ğŸ¯ æœ€ä½³æ›¿ä»£æ–¹æ¡ˆ:
1. ğŸ  åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼ - img2imgåŠŸèƒ½å®Œå…¨æ”¯æŒ
2. ğŸ–¼ï¸ ä½¿ç”¨ControlNetæ¨¡å¼:
   â€¢ é€‰æ‹©Cannyè¾¹ç¼˜æ£€æµ‹
   â€¢ ä¸Šä¼ æ‚¨çš„å›¾åƒä½œä¸ºæ§åˆ¶å›¾
   â€¢ è¾“å…¥ç›¸åŒçš„æç¤ºè¯
   â€¢ è·å¾—ç±»ä¼¼çš„å›¾åƒå˜æ¢æ•ˆæœ

ğŸ“‹ æ“ä½œæ­¥éª¤:
â€¢ ç‚¹å‡»ä¸Šæ–¹"åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼"æŒ‰é’®
â€¢ æˆ–åˆ‡æ¢åˆ°"ControlNetç”Ÿæˆ"æ ‡ç­¾é¡µ
â€¢ é€‰æ‹©"canny"ç±»å‹ï¼Œä¸Šä¼ åŒä¸€å¼ å›¾åƒ

ğŸ’¡ ä¸ºä»€ä¹ˆä¼šè¿™æ ·:
{status}

ğŸ”§ æŠ€æœ¯åŸå› :
â€¢ Hugging Faceå…¬å…±APIä¸»è¦æ”¯æŒtext-to-image
â€¢ img2imgéœ€è¦ä¸“é—¨çš„APIç«¯ç‚¹æ”¯æŒ
â€¢ å¤§å¤šæ•°æ¨¡å‹å°šæœªæä¾›img2img APIæ¥å£"""
                
                return None, fallback_message
                
        except Exception as e:
            error_message = f"""âŒ img2img APIè°ƒç”¨å¼‚å¸¸: {str(e)}

ğŸ”„ æ¨èè§£å†³æ–¹æ¡ˆ:
1. ğŸ  åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼ (100%æ”¯æŒimg2img)
2. ğŸ–¼ï¸ ä½¿ç”¨ControlNetæ¨¡å¼æ›¿ä»£
3. ğŸ¨ ä½¿ç”¨çº¯æ–‡ç”Ÿå›¾æ¨¡å¼

ğŸ“‹ å¿«é€Ÿæ“ä½œ:
â€¢ ç‚¹å‡»"åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å¼"
â€¢ æˆ–ä½¿ç”¨ControlNetçš„CannyåŠŸèƒ½"""
            
            return None, error_message
    
    else:
        # æœ¬åœ°æ¨¡å¼ - å®Œå…¨æ”¯æŒimg2img
        try:
            print(f"ğŸ  æœ¬åœ°æ¨¡å¼: å¼€å§‹img2imgç”Ÿæˆ...")
            print(f"   è¾“å…¥å›¾åƒå°ºå¯¸: {input_image.size}")
            print(f"   å˜æ¢å¼ºåº¦: {strength}")
            print(f"   ç”Ÿæˆæ­¥æ•°: {num_steps}")
            
            # è®¾ç½®éšæœºç§å­
            if seed != -1:
                generator = torch.Generator(device=device).manual_seed(seed)
                print(f"   ä½¿ç”¨ç§å­: {seed}")
            else:
                generator = None
                print(f"   éšæœºç§å­")
                
            # ç”Ÿæˆå›¾åƒ
            with torch.autocast(device):
                result = img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    image=input_image,
                    strength=strength,
                    num_inference_steps=num_steps,
                    guidance_scale=guidance_scale,
                    generator=generator
                )
            
            image = result.images[0]
            
            success_message = f"""âœ… æœ¬åœ°img2imgç”ŸæˆæˆåŠŸï¼
ğŸ–¼ï¸ è¾“å‡ºå°ºå¯¸: {image.size}
ğŸ¯ å˜æ¢å¼ºåº¦: {strength}
ğŸ“ ç”Ÿæˆæ­¥æ•°: {num_steps}
âš¡ å¼•å¯¼å¼ºåº¦: {guidance_scale}"""
            
            return image, success_message
            
        except Exception as e:
            error_message = f"""âŒ æœ¬åœ°img2imgç”Ÿæˆå¤±è´¥: {str(e)}

ğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:
1. ğŸ“‰ é™ä½å›¾åƒåˆ†è¾¨ç‡ (512x512)
2. ğŸ“Š å‡å°‘ç”Ÿæˆæ­¥æ•° (15-25)
3. ğŸšï¸ è°ƒæ•´å˜æ¢å¼ºåº¦ (0.5-0.8)
4. ğŸ’¾ æ£€æŸ¥æ˜¾å­˜æ˜¯å¦å……è¶³
5. ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹

âš ï¸ å¦‚æœé—®é¢˜æŒç»­ï¼Œå»ºè®®ä½¿ç”¨ControlNetæ¨¡å¼"""
            
            return None, error_message

def add_prompt_tags(current_prompt, selected_tags):
    """æ·»åŠ é€‰ä¸­çš„æ ‡ç­¾åˆ°promptä¸­"""
    if not selected_tags:
        return current_prompt
    
    # å°†é€‰ä¸­çš„æ ‡ç­¾åˆå¹¶
    new_tags = ", ".join(selected_tags)
    
    if current_prompt:
        # å¦‚æœå·²æœ‰promptï¼Œåˆ™æ·»åŠ åˆ°æœ«å°¾
        return f"{current_prompt}, {new_tags}"
    else:
        # å¦‚æœæ²¡æœ‰promptï¼Œç›´æ¥ä½¿ç”¨æ ‡ç­¾
        return new_tags

def get_current_model_info():
    """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
    global current_model
    if current_model:
        model_name = MODELS.get(current_model, current_model)
        return f"ğŸ“¦ å½“å‰æ¨¡å‹: {model_name}"
    else:
        return "âŒ æœªåŠ è½½æ¨¡å‹"

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="ğŸ¨ AI å›¾åƒç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¨ AI å›¾åƒç”Ÿæˆå™¨ Pro
        
        æ”¯æŒå¤šç§æ¨¡å‹å’Œä¸‰ç§ç”Ÿæˆæ¨¡å¼ï¼š
        - **ğŸ“ æ–‡ç”Ÿå›¾æ¨¡å¼**ï¼šçº¯æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒ
        - **ğŸ”„ ä¼ ç»Ÿå›¾ç”Ÿå›¾**ï¼šå›¾åƒé£æ ¼è½¬æ¢
        - **ğŸ–¼ï¸ ControlNetæ¨¡å¼**ï¼šç²¾ç¡®ç»“æ„æ§åˆ¶çš„å›¾åƒç”Ÿæˆ
        
        > åŸºäº Stable Diffusion ç³»åˆ—æ¨¡å‹ + ControlNet
        """)
        
        # æ¨¡å¼è¯´æ˜ä¿¡æ¯é¢æ¿
        gr.Markdown("""
        ### ğŸš€ è¿è¡Œæ¨¡å¼è¯´æ˜
        - **ğŸŒ APIæ¨¡å¼ (æ¨è)**: é€šè¿‡ Hugging Face API åœ¨çº¿ç”Ÿæˆï¼Œ**æ— éœ€ä¸‹è½½ä»»ä½•æ¨¡å‹**ï¼ŒèŠ‚çœ 4-10GB å­˜å‚¨ç©ºé—´ï¼
        - **ğŸ’» æœ¬åœ°æ¨¡å¼**: ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°è¿è¡Œï¼Œéœ€è¦ 4-10GB å­˜å‚¨ç©ºé—´ï¼Œä½†è¿è¡Œé€Ÿåº¦æ›´å¿«ï¼Œæ”¯æŒæ›´å¤šè‡ªå®šä¹‰å‚æ•°
        - **ğŸ”‘ API Token**: APIæ¨¡å¼éœ€è¦ [Hugging Face Token](https://huggingface.co/settings/tokens) (å…è´¹è´¦æˆ·å³å¯)
        
        ### ğŸ¯ æ¨èAPIæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰
        1. **FLUX.1 Dev** - ğŸ¥‡ æœ€å¼ºå¤§çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œè´¨é‡æé«˜
        2. **FLUX.1 Schnell** - âš¡ å¿«é€Ÿç”Ÿæˆï¼Œé«˜è´¨é‡è¾“å‡º
        3. **SDXL Base 1.0** - ğŸ¨ é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼Œç»å…¸é€‰æ‹©
        4. **SD 3.5 Large** - ğŸš€ æœ€æ–°ç‰ˆæœ¬ï¼Œæ–‡æœ¬ç†è§£èƒ½åŠ›å¼º
        5. **Kolors** - ğŸ“¸ é€¼çœŸå›¾åƒç”Ÿæˆï¼Œäººåƒæ•ˆæœä½³
        """)
        
        # æ¨¡å‹é€‰æ‹©å’ŒåŠ è½½åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=3):
                # è¿è¡Œæ¨¡å¼é€‰æ‹©
                run_mode_radio = gr.Radio(
                    choices=[
                        ("ğŸŒ APIæ¨¡å¼ (æ¨è) - æ— éœ€ä¸‹è½½ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´", "api"),
                        ("ğŸ’» æœ¬åœ°æ¨¡å¼ - ä¸‹è½½åˆ°æœ¬åœ°ï¼Œéœ€è¦å¤§é‡å­˜å‚¨ç©ºé—´", "local")
                    ],
                    value="api",
                    label="âš™ï¸ è¿è¡Œæ¨¡å¼",
                    info="APIæ¨¡å¼é€šè¿‡ç½‘ç»œè°ƒç”¨ï¼Œæœ¬åœ°æ¨¡å¼éœ€è¦ä¸‹è½½4-10GBæ¨¡å‹æ–‡ä»¶"
                )
                
                model_dropdown = gr.Dropdown(
                    choices=[(name, model_id) for model_id, name in API_SUPPORTED_MODELS.items()],
                    value="black-forest-labs/FLUX.1-dev",
                    label="ğŸ¤– é€‰æ‹©åŸºç¡€æ¨¡å‹ (ä»…APIæ”¯æŒçš„æ¨¡å‹)",
                    info="âœ… APIæ¨¡å¼ - è¿™äº›æ¨¡å‹æ”¯æŒäº‘ç«¯æ¨ç†ï¼Œæ— éœ€ä¸‹è½½"
                )
                controlnet_dropdown = gr.Dropdown(
                    choices=[(f"{info['name']} - {info['description']}", key) for key, info in CONTROLNET_TYPES.items()],
                    value="canny",
                    label="ğŸ® é€‰æ‹©ControlNetç±»å‹",
                    info="é€‰æ‹©ä¸åŒçš„æ§åˆ¶æ–¹å¼"
                )
                
                # API Token è®¾ç½®
                with gr.Accordion("ğŸ”‘ APIè®¾ç½® (APIæ¨¡å¼å¿…çœ‹)", open=True):
                    gr.Markdown("""
                    **ğŸ¯ è·å–å…è´¹API Tokenï¼š**
                    1. è®¿é—® [Hugging Face](https://huggingface.co/settings/tokens)
                    2. åˆ›å»ºæ–°Token (Readæƒé™å³å¯)
                    3. å¤åˆ¶å¹¶ç²˜è´´åˆ°ä¸‹æ–¹è¾“å…¥æ¡†
                    
                    **ğŸ’¡ æç¤ºï¼š** å…è´¹è´¦æˆ·æ¯æœˆæœ‰ä¸€å®šè°ƒç”¨é™åˆ¶ï¼Œä»˜è´¹è´¦æˆ·æ— é™åˆ¶ä¸”é€Ÿåº¦æ›´å¿«
                    """)
                    api_token_input = gr.Textbox(
                        label="ğŸ”‘ Hugging Face API Token",
                        placeholder="hf_xxxxxxxxxxxxxxxxxxxx (å»ºè®®è®¾ç½®ï¼Œå¦åˆ™å¯èƒ½é‡åˆ°é™æµ)",
                        type="password",
                        info="ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è·å–å…è´¹Tokenï¼Œæå‡APIè°ƒç”¨ç¨³å®šæ€§"
                    )
                    
                    # API Token éªŒè¯çŠ¶æ€
                    token_status = gr.Textbox(
                        label="TokenéªŒè¯çŠ¶æ€",
                        value="âš ï¸ è¯·è¾“å…¥API Tokenè¿›è¡ŒéªŒè¯",
                        interactive=False,
                        lines=1
                    )
                    
                    # æ¨¡å‹APIæ”¯æŒçŠ¶æ€
                    model_api_status = gr.Textbox(
                        label="æ¨¡å‹APIæ”¯æŒçŠ¶æ€",
                        value="âœ… å½“å‰æ¨¡å‹æ”¯æŒAPIæ¨¡å¼",
                        interactive=False,
                        lines=1
                    )
                    
                    # APIè¿æ¥æµ‹è¯•æŒ‰é’®
                    test_api_btn = gr.Button("ğŸ”— æµ‹è¯•åŸºç¡€æ¨¡å‹APIè¿æ¥", variant="secondary")
                    
                    # ControlNet API æµ‹è¯•
                    with gr.Row():
                        controlnet_test_dropdown = gr.Dropdown(
                            choices=list(CONTROLNET_TYPES.keys()),
                            value="canny",
                            label="é€‰æ‹©ControlNetç±»å‹è¿›è¡Œæµ‹è¯•",
                            scale=2
                        )
                        test_controlnet_api_btn = gr.Button("ğŸ® æµ‹è¯•ControlNet API", variant="secondary", scale=1)
                    
                    controlnet_api_status = gr.Textbox(
                        label="ControlNet APIæµ‹è¯•çŠ¶æ€",
                        value="ç‚¹å‡»æµ‹è¯•æŒ‰é’®æ£€æŸ¥ControlNet APIè¿æ¥",
                        interactive=False,
                        lines=1
                    )
                
                # ä»£ç†è®¾ç½®
                with gr.Accordion("ğŸŒ ç½‘ç»œä»£ç†è®¾ç½® (è§£å†³è¿æ¥è¶…æ—¶é—®é¢˜)", open=False):
                    gr.Markdown("""
                    **ğŸš¨ å¦‚æœé‡åˆ° "API call timeout" é”™è¯¯ï¼Œè¯·å¯ç”¨ä»£ç†ï¼š**
                    
                    **Clash ä»£ç†è®¾ç½®ï¼š**
                    - HTTPä»£ç†ç«¯å£é€šå¸¸æ˜¯ï¼š`http://127.0.0.1:7890`
                    - HTTPSä»£ç†ç«¯å£é€šå¸¸æ˜¯ï¼š`http://127.0.0.1:7890`
                    - å¦‚æœç«¯å£ä¸åŒï¼Œè¯·æŸ¥çœ‹ Clash çš„ç«¯å£è®¾ç½®
                    
                    **å…¶ä»–ä»£ç†è½¯ä»¶ï¼š**
                    - V2Ray: `http://127.0.0.1:10809`
                    - Shadowsocks: `http://127.0.0.1:1080`
                    - è¯·æ ¹æ®æ‚¨çš„ä»£ç†è½¯ä»¶å®é™…ç«¯å£å¡«å†™
                    """)
                    
                    proxy_enabled = gr.Checkbox(
                        label="å¯ç”¨ä»£ç†",
                        value=False,
                        info="å¦‚æœç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·å¯ç”¨æ­¤é€‰é¡¹"
                    )
                    
                    with gr.Row():
                        http_proxy_input = gr.Textbox(
                            label="HTTP ä»£ç†",
                            placeholder="http://127.0.0.1:7890",
                            info="å¡«å†™ HTTP ä»£ç†åœ°å€å’Œç«¯å£"
                        )
                        https_proxy_input = gr.Textbox(
                            label="HTTPS ä»£ç†", 
                            placeholder="http://127.0.0.1:7890",
                            info="å¡«å†™ HTTPS ä»£ç†åœ°å€å’Œç«¯å£"
                        )
                    
                    proxy_status = gr.Textbox(
                        label="ä»£ç†çŠ¶æ€",
                        value="âŒ ä»£ç†å·²ç¦ç”¨",
                        interactive=False
                    )
                    
                    test_proxy_btn = gr.Button("ğŸ”— æµ‹è¯•ä»£ç†è¿æ¥", variant="secondary")
                    
                    def test_proxy_connection(enabled, http_proxy, https_proxy):
                        """æµ‹è¯•ä»£ç†è¿æ¥"""
                        if not enabled:
                            return "âŒ ä»£ç†æœªå¯ç”¨ï¼Œæ— æ³•æµ‹è¯•"
                        
                        if not (http_proxy or https_proxy):
                            return "âŒ è¯·å¡«å†™ä»£ç†åœ°å€"
                        
                        proxies = {}
                        if http_proxy:
                            proxies["http"] = http_proxy
                        if https_proxy:
                            proxies["https"] = https_proxy
                        
                        try:
                            # æµ‹è¯•è¿æ¥åˆ° Hugging Face
                            response = requests.get(
                                "https://huggingface.co", 
                                proxies=proxies, 
                                timeout=10
                            )
                            if response.status_code == 200:
                                return "âœ… ä»£ç†è¿æ¥æµ‹è¯•æˆåŠŸï¼"
                            else:
                                return f"âš ï¸ ä»£ç†è¿æ¥æµ‹è¯•å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                        except Exception as e:
                            return f"âŒ ä»£ç†è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
                    
                    test_proxy_btn.click(
                        test_proxy_connection,
                        inputs=[proxy_enabled, http_proxy_input, https_proxy_input],
                        outputs=[proxy_status]
                    )
                    
                load_btn = gr.Button("ğŸš€ åŠ è½½é€‰ä¸­æ¨¡å‹", variant="primary", size="lg")
            with gr.Column(scale=2):
                current_model_display = gr.Textbox(
                    label="å½“å‰æ¨¡å‹çŠ¶æ€", 
                    value="ğŸ“¦ é»˜è®¤æ¨¡å‹: Stable Diffusion v1.5\nğŸ® é»˜è®¤ControlNet: Cannyè¾¹ç¼˜æ£€æµ‹\nâš™ï¸ é»˜è®¤æ¨¡å¼: APIæ¨¡å¼",
                    interactive=False,
                    lines=3
                )
        
        load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", value="é€‰æ‹©æ¨¡å‹åç‚¹å‡»åŠ è½½å¼€å§‹ä½¿ç”¨", lines=3)
        
        # GitHub è‡ªåŠ¨æ¨é€åŒºåŸŸ
        with gr.Accordion("ğŸš€ GitHub è‡ªåŠ¨æ¨é€", open=False):
            gr.Markdown("""
            **ğŸ“¦ ä»£ç åŒæ­¥åŠŸèƒ½ï¼š**
            - è‡ªåŠ¨å°†å½“å‰æ‰€æœ‰æ›´æ”¹æ¨é€åˆ° GitHub ä»“åº“
            - åŒ…å«ä»£ç æ›´æ–°ã€æ–°å¢æ–‡ä»¶ã€é…ç½®ä¿®æ”¹ç­‰
            - é€‚åˆå¼€å‘è¿‡ç¨‹ä¸­çš„ç‰ˆæœ¬å¤‡ä»½å’ŒåŒæ­¥
            
            **âš ï¸ æ³¨æ„äº‹é¡¹ï¼š**
            - ç¡®ä¿å·²é…ç½® GitHub è®¿é—®æƒé™
            - å»ºè®®åœ¨é‡è¦åŠŸèƒ½å®Œæˆåä½¿ç”¨
            - æ¨é€å‰ä¼šè‡ªåŠ¨æ·»åŠ æ‰€æœ‰æ›´æ”¹æ–‡ä»¶
            """)
            
            with gr.Row():
                push_to_github_btn = gr.Button("ğŸš€ æ¨é€åˆ° GitHub", variant="primary", size="lg")
                github_status = gr.Textbox(
                    label="æ¨é€çŠ¶æ€",
                    value="ç‚¹å‡»æŒ‰é’®å°†ä»£ç æ¨é€åˆ° GitHub ä»“åº“",
                    interactive=False,
                    lines=2
                )
        
        
        # Prompt è¾…åŠ©é€‰æ‹©å™¨
        with gr.Accordion("ğŸ¯ Prompt è¾…åŠ©é€‰æ‹©å™¨", open=False):
            gr.Markdown("### ğŸ’¡ é€‰æ‹©è¯æ¡å¿«é€Ÿæ„å»ºé«˜è´¨é‡æç¤ºè¯")
            
            # æ­£é¢è¯æ¡
            gr.Markdown("#### âœ¨ æ­£é¢æç¤ºè¯ (Positive Prompt)")
            with gr.Row():
                with gr.Column():
                    quality_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["è´¨é‡å¢å¼º"],
                        label="ğŸŒŸ è´¨é‡å¢å¼º",
                        info="æå‡å›¾åƒè´¨é‡å’Œç»†èŠ‚"
                    )
                    style_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["è‰ºæœ¯é£æ ¼"],
                        label="ğŸ¨ è‰ºæœ¯é£æ ¼",
                        info="é€‰æ‹©è‰ºæœ¯è¡¨ç°å½¢å¼"
                    )
                with gr.Column():
                    lighting_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["å…‰ç…§æ•ˆæœ"],
                        label="ğŸ’¡ å…‰ç…§æ•ˆæœ",
                        info="è®¾ç½®å…‰ç…§å’Œæ°›å›´"
                    )
                    composition_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["æ„å›¾è§†è§’"],
                        label="ğŸ“ æ„å›¾è§†è§’",
                        info="é€‰æ‹©æ‹æ‘„è§’åº¦å’Œæ„å›¾"
                    )
                with gr.Column():
                    mood_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["æƒ…ç»ªæ°›å›´"],
                        label="ğŸ˜Š æƒ…ç»ªæ°›å›´",
                        info="è®¾å®šç”»é¢æƒ…æ„Ÿè‰²è°ƒ"
                    )
                    scene_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["ç¯å¢ƒåœºæ™¯"],
                        label="ğŸŒ ç¯å¢ƒåœºæ™¯",
                        info="é€‰æ‹©èƒŒæ™¯å’Œç¯å¢ƒ"
                    )
                with gr.Column():
                    color_tags = gr.CheckboxGroup(
                        choices=PROMPT_CATEGORIES["è‰²å½©é£æ ¼"],
                        label="ğŸ¨ è‰²å½©é£æ ¼",
                        info="è®¾å®šè‰²å½©ä¸»è°ƒ"
                    )
            
            # è´Ÿé¢è¯æ¡
            gr.Markdown("#### ğŸš« è´Ÿé¢æç¤ºè¯ (Negative Prompt)")
            with gr.Row():
                with gr.Column():
                    neg_quality_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["ç”»è´¨é—®é¢˜"],
                        label="ğŸš« ç”»è´¨é—®é¢˜",
                        info="é¿å…ç”»è´¨ç›¸å…³é—®é¢˜"
                    )
                    neg_anatomy_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["è§£å‰–é”™è¯¯"],
                        label="ğŸš« è§£å‰–é”™è¯¯",
                        info="é¿å…èº«ä½“ç»“æ„é”™è¯¯"
                    )
                with gr.Column():
                    neg_face_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["é¢éƒ¨é—®é¢˜"],
                        label="ğŸš« é¢éƒ¨é—®é¢˜",
                        info="é¿å…é¢éƒ¨ç›¸å…³é—®é¢˜"
                    )
                    neg_style_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["è‰ºæœ¯é£æ ¼"],
                        label="ğŸš« é¿å…é£æ ¼",
                        info="æ’é™¤ä¸æƒ³è¦çš„è‰ºæœ¯é£æ ¼"
                    )
                with gr.Column():
                    neg_tech_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["æŠ€æœ¯é—®é¢˜"],
                        label="ğŸš« æŠ€æœ¯é—®é¢˜",
                        info="é¿å…æ°´å°ã€è£å‰ªç­‰æŠ€æœ¯é—®é¢˜"
                    )
                    neg_lighting_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["å…‰ç…§é—®é¢˜"],
                        label="ğŸš« å…‰ç…§é—®é¢˜",
                        info="é¿å…å…‰ç…§ç›¸å…³é—®é¢˜"
                    )
                with gr.Column():
                    neg_composition_tags = gr.CheckboxGroup(
                        choices=NEGATIVE_PROMPT_CATEGORIES["æ„å›¾é—®é¢˜"],
                        label="ğŸš« æ„å›¾é—®é¢˜",
                        info="é¿å…æ„å›¾ç›¸å…³é—®é¢˜"
                    )
            
            with gr.Row():
                clear_tags_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰é€‰æ‹©", variant="secondary")
                apply_positive_tags_btn = gr.Button("âœ¨ åº”ç”¨æ­£é¢è¯æ¡åˆ°å½“å‰æ ‡ç­¾é¡µ", variant="primary")
                apply_negative_tags_btn = gr.Button("ğŸš« åº”ç”¨è´Ÿé¢è¯æ¡åˆ°å½“å‰æ ‡ç­¾é¡µ", variant="secondary")
        
        with gr.Tabs() as tabs:
            # Tab 1: åŸºç¡€æ–‡ç”Ÿå›¾
            with gr.TabItem("ğŸ“ æ–‡ç”Ÿå›¾æ¨¡å¼"):
                with gr.Row():
                    with gr.Column(scale=1):
                        with gr.Row():
                            prompt1 = gr.Textbox(
                                label="æç¤ºè¯ (Prompt)",
                                placeholder="æè¿°ä½ æƒ³è¦çš„å›¾åƒï¼Œä¾‹å¦‚ï¼ša beautiful landscape with mountains and lakes, highly detailed, 4k",
                                lines=3,
                                scale=4
                            )
                            with gr.Column(scale=1):
                                apply_positive_to_prompt1 = gr.Button("â• æ­£é¢è¯æ¡", variant="secondary", size="sm")
                                apply_negative_to_prompt1 = gr.Button("â– è´Ÿé¢è¯æ¡", variant="secondary", size="sm")
                        
                        negative_prompt1 = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯ (Negative Prompt)",
                            placeholder="ä¸æƒ³è¦çš„å…ƒç´ ï¼Œä¾‹å¦‚ï¼šblurry, low quality, distorted",
                            lines=2
                        )
                        
                        with gr.Row():
                            num_steps1 = gr.Slider(10, 50, value=20, step=1, label="é‡‡æ ·æ­¥æ•°")
                            guidance_scale1 = gr.Slider(1, 20, value=7.5, step=0.5, label="å¼•å¯¼å¼ºåº¦")
                        
                        with gr.Row():
                            width1 = gr.Slider(256, 1024, value=512, step=64, label="å®½åº¦")
                            height1 = gr.Slider(256, 1024, value=512, step=64, label="é«˜åº¦")
                        
                        seed1 = gr.Number(label="éšæœºç§å­ (-1ä¸ºéšæœº)", value=-1)
                        generate_btn1 = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary")
                    
                    with gr.Column(scale=1):
                        output_image1 = gr.Image(label="ç”Ÿæˆçš„å›¾åƒ", type="pil")
                        output_status1 = gr.Textbox(label="ç”ŸæˆçŠ¶æ€")
            
            # Tab 2: ä¼ ç»Ÿå›¾ç”Ÿå›¾
            with gr.TabItem("ğŸ”„ ä¼ ç»Ÿå›¾ç”Ÿå›¾"):
                # æ·»åŠ æŠ€æœ¯è¯´æ˜
                with gr.Accordion("ğŸ”¬ æŠ€æœ¯åŸç†è¯´æ˜", open=False):
                    gr.Markdown("""
### ğŸ“š img2imgæŠ€æœ¯åŸç†

**img2img** æ˜¯ä¸€ä¸ªå¤æ‚çš„å›¾åƒå¤„ç†æµç¨‹ï¼Œéœ€è¦å®Œæ•´çš„AIæ¨¡å‹ç®¡é“ï¼š

ğŸ”„ **å®Œæ•´æµç¨‹**ï¼š
1. **VAEç¼–ç å™¨** â†’ å°†è¾“å…¥å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´ (latent space)
2. **å™ªå£°æ·»åŠ ** â†’ æ ¹æ®strengthå‚æ•°æ·»åŠ ä¸åŒç¨‹åº¦çš„å™ªå£°
3. **UNeté‡‡æ ·** â†’ åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œå»å™ªè¿‡ç¨‹ï¼Œç»“åˆæ–‡æœ¬æç¤º
4. **VAEè§£ç å™¨** â†’ å°†æ½œåœ¨è¡¨ç¤ºè§£ç å›å›¾åƒç©ºé—´

ğŸ  **æœ¬åœ°æ¨¡å¼** vs ğŸŒ **APIæ¨¡å¼**ï¼š
- **æœ¬åœ°æ¨¡å¼**ï¼šâœ… å®Œæ•´æ”¯æŒï¼ŒåŒ…å«VAEç¼–ç å™¨/è§£ç å™¨
- **APIæ¨¡å¼**ï¼šâŒ é™åˆ¶è¾ƒå¤§ï¼Œå…¬å…±APIé€šå¸¸åªæä¾›ç®€åŒ–æ¥å£

ğŸ’¡ **ComfyUIå¯¹æ¯”**ï¼š
ComfyUIä½¿ç”¨å®Œæ•´çš„å·¥ä½œæµèŠ‚ç‚¹ï¼ŒåŒ…å«ç‹¬ç«‹çš„VAEç¼–ç å™¨èŠ‚ç‚¹ï¼Œ
è¿™æ­£æ˜¯å®ç°é«˜è´¨é‡img2imgçš„å…³é”®ç»„ä»¶ã€‚

ğŸ¯ **æœ€ä½³æ›¿ä»£æ–¹æ¡ˆ**ï¼š
å¦‚æœæ‚¨åœ¨APIæ¨¡å¼ä¸‹éœ€è¦ç±»ä¼¼æ•ˆæœï¼Œæ¨èä½¿ç”¨ **ControlNet + Cannyè¾¹ç¼˜æ£€æµ‹**
                    """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        input_image = gr.Image(label="ä¸Šä¼ è¾“å…¥å›¾åƒ", type="pil")
                        
                        with gr.Row():
                            prompt_img2img = gr.Textbox(
                                label="æç¤ºè¯ (Prompt)",
                                placeholder="æè¿°æƒ³è¦çš„é£æ ¼å˜åŒ–ï¼Œä¾‹å¦‚ï¼šoil painting style, vibrant colors",
                                lines=3,
                                scale=4
                            )
                            with gr.Column(scale=1):
                                apply_positive_to_img2img = gr.Button("â• æ­£é¢è¯æ¡", variant="secondary", size="sm")
                                apply_negative_to_img2img = gr.Button("â– è´Ÿé¢è¯æ¡", variant="secondary", size="sm")
                        
                        negative_prompt_img2img = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯ (Negative Prompt)",
                            placeholder="ä¸æƒ³è¦çš„å…ƒç´ ",
                            lines=2
                        )
                        
                        strength = gr.Slider(0.1, 1.0, value=0.7, step=0.1, label="å˜åŒ–å¼ºåº¦ (è¶Šé«˜å˜åŒ–è¶Šå¤§)")
                        
                        with gr.Row():
                            num_steps_img2img = gr.Slider(10, 50, value=20, step=1, label="é‡‡æ ·æ­¥æ•°")
                            guidance_scale_img2img = gr.Slider(1, 20, value=7.5, step=0.5, label="å¼•å¯¼å¼ºåº¦")
                        
                        with gr.Row():
                            width_img2img = gr.Slider(256, 1024, value=512, step=64, label="å®½åº¦")
                            height_img2img = gr.Slider(256, 1024, value=512, step=64, label="é«˜åº¦")
                        
                        seed_img2img = gr.Number(label="éšæœºç§å­ (-1ä¸ºéšæœº)", value=-1)
                        
                        with gr.Row():
                            generate_btn_img2img = gr.Button("ğŸ”„ ä¼ ç»Ÿå›¾ç”Ÿå›¾", variant="secondary", scale=3)
                            test_img2img_api_btn = gr.Button("ğŸ§ª æµ‹è¯•API", variant="secondary", scale=1, 
                                                           visible=True, size="sm")
                    
                    with gr.Column(scale=1):
                        output_image_img2img = gr.Image(label="ç”Ÿæˆçš„å›¾åƒ", type="pil")
                        output_status_img2img = gr.Textbox(label="ç”ŸæˆçŠ¶æ€")
            
            # Tab 3: ControlNetå›¾åƒå¼•å¯¼
            with gr.TabItem("ğŸ–¼ï¸ ControlNetæ¨¡å¼"):
                with gr.Row():
                    with gr.Column(scale=1):
                        control_image = gr.Image(label="ä¸Šä¼ æ§åˆ¶å›¾åƒ", type="pil")
                        
                        control_type_radio = gr.Radio(
                            choices=[(f"{info['name']} - {info['description']}", key) for key, info in CONTROLNET_TYPES.items()],
                            value="canny",
                            label="ğŸ® æ§åˆ¶ç±»å‹",
                            info="é€‰æ‹©æ§åˆ¶æ–¹å¼ï¼ˆéœ€è¦ä¸åŠ è½½çš„ControlNetç±»å‹ä¸€è‡´ï¼‰"
                        )
                        
                        with gr.Row():
                            prompt2 = gr.Textbox(
                                label="æç¤ºè¯ (Prompt)",
                                placeholder="åŸºäºä¸Šä¼ å›¾åƒçš„ç»“æ„ï¼Œæè¿°æƒ³è¦çš„é£æ ¼ï¼Œä¾‹å¦‚ï¼šoil painting style, sunset colors",
                                lines=3,
                                scale=4
                            )
                            with gr.Column(scale=1):
                                apply_positive_to_prompt2 = gr.Button("â• æ­£é¢è¯æ¡", variant="secondary", size="sm")
                                apply_negative_to_prompt2 = gr.Button("â– è´Ÿé¢è¯æ¡", variant="secondary", size="sm")
                        
                        negative_prompt2 = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯ (Negative Prompt)",
                            placeholder="ä¸æƒ³è¦çš„å…ƒç´ ",
                            lines=2
                        )
                        
                        with gr.Row():
                            num_steps2 = gr.Slider(10, 50, value=20, step=1, label="é‡‡æ ·æ­¥æ•°")
                            guidance_scale2 = gr.Slider(1, 20, value=7.5, step=0.5, label="å¼•å¯¼å¼ºåº¦")
                        
                        controlnet_scale = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label="ControlNetå¼ºåº¦")
                        
                        with gr.Row():
                            width2 = gr.Slider(256, 1024, value=512, step=64, label="å®½åº¦")
                            height2 = gr.Slider(256, 1024, value=512, step=64, label="é«˜åº¦")
                        
                        seed2 = gr.Number(label="éšæœºç§å­ (-1ä¸ºéšæœº)", value=-1)
                        generate_btn2 = gr.Button("ğŸ¨ ControlNetç”Ÿæˆ", variant="primary")
                    
                    with gr.Column(scale=1):
                        with gr.Row():
                            control_preview = gr.Image(label="æ§åˆ¶å›¾åƒé¢„è§ˆ", type="pil")
                            output_image2 = gr.Image(label="ç”Ÿæˆçš„å›¾åƒ", type="pil")
                        output_status2 = gr.Textbox(label="ç”ŸæˆçŠ¶æ€")
        
        # ç¤ºä¾‹å’Œå¯¹æ¯”è¯´æ˜
        gr.Markdown("""
        ## ğŸ’¡ ä¸‰ç§æ¨¡å¼å¯¹æ¯”ä¸ä½¿ç”¨æŒ‡å—
        
        ### ï¿½ **è¿è¡Œæ¨¡å¼è¯¦ç»†å¯¹æ¯”**
        
        | è¿è¡Œæ¨¡å¼ | å­˜å‚¨ç©ºé—´ | åˆå§‹åŒ–æ—¶é—´ | ç”Ÿæˆé€Ÿåº¦ | ç½‘ç»œè¦æ±‚ | æˆæœ¬ | æ¨èæŒ‡æ•° |
        |----------|----------|------------|----------|----------|------|----------|
        | **ğŸŒ APIæ¨¡å¼** | **0 GB** | å³æ—¶ | ä¸­ç­‰ | éœ€è¦ç½‘ç»œ | å…è´¹é¢åº¦+ä»˜è´¹ | â­â­â­â­â­ |
        | **ğŸ’» æœ¬åœ°æ¨¡å¼** | **4-10 GB** | 5-15åˆ†é’Ÿ | å¿«é€Ÿ | ä»…ä¸‹è½½æ—¶éœ€è¦ | ç¡¬ä»¶æˆæœ¬ | â­â­â­ |
        
        **ğŸ¯ æ¨¡å‹å­˜å‚¨ç©ºé—´è¯¦æƒ… (æœ¬åœ°æ¨¡å¼)ï¼š**
        - åŸºç¡€æ¨¡å‹ (SD v1.5): ~4GB
        - ControlNet æ¨¡å‹: ~1.5GB æ¯ä¸ª
        - é«˜çº§æ¨¡å‹ (SD v2.1): ~5-6GB
        - å®Œæ•´é…ç½®æ€»è®¡: **6-10GB**
        
        **ğŸ’¡ å»ºè®®ï¼š**
        - ğŸŸ¢ **å­˜å‚¨ç©ºé—´ç´§å¼ ** â†’ é€‰æ‹©APIæ¨¡å¼
        - ğŸŸ¢ **éœ€è¦é¢‘ç¹ç”Ÿæˆ** â†’ é€‰æ‹©æœ¬åœ°æ¨¡å¼  
        - ğŸŸ¢ **åˆæ¬¡ä½“éªŒ** â†’ å»ºè®®APIæ¨¡å¼
        
        ### ï¿½ğŸ” **ç”Ÿæˆæ¨¡å¼å¯¹æ¯”è¡¨**
        
        | æ¨¡å¼ | è¾“å…¥ | æ§åˆ¶æ–¹å¼ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
        |------|------|----------|------|----------|
        | **ğŸ“ æ–‡ç”Ÿå›¾** | ä»…æ–‡æœ¬ | æç¤ºè¯ | å®Œå…¨åˆ›æ–°ï¼Œæ— é™å¯èƒ½ | åŸåˆ›ä½œå“ï¼Œæ¦‚å¿µè®¾è®¡ |
        | **ğŸ”„ ä¼ ç»Ÿå›¾ç”Ÿå›¾** | å›¾ç‰‡+æ–‡æœ¬ | strengthå‚æ•° | å¿«é€Ÿé£æ ¼è½¬æ¢ | ç®€å•é£æ ¼åŒ–ï¼Œå¿«é€Ÿä¿®æ”¹ |
        | **ğŸ–¼ï¸ ControlNet** | å›¾ç‰‡+æ–‡æœ¬ | ç²¾ç¡®ç»“æ„æ§åˆ¶ | ä¿æŒç»“æ„ï¼Œç²¾ç¡®æ§åˆ¶ | å»ºç­‘é‡è®¾è®¡ï¼Œå§¿æ€ä¿æŒ |
        
        ### ğŸ“ **æ–‡ç”Ÿå›¾æ¨¡å¼ç¤ºä¾‹ï¼š**
        - `a majestic dragon flying over a medieval castle, fantasy art, highly detailed`
        - `portrait of a young woman, oil painting style, soft lighting, renaissance art`
        
        ### ğŸ¯ **Prompt è¾…åŠ©å™¨ä½¿ç”¨è¯´æ˜ï¼š**
        - **âœ¨ æ­£é¢è¯æ¡**ï¼šæè¿°ä½ æƒ³è¦çš„æ•ˆæœã€é£æ ¼ã€è´¨é‡ç­‰
        - **ğŸš« è´Ÿé¢è¯æ¡**ï¼šæè¿°ä½ ä¸æƒ³è¦çš„é—®é¢˜ã€é£æ ¼ã€ç‘•ç–µç­‰
        - **ğŸ“ åº”ç”¨æ–¹å¼**ï¼šç‚¹å‡» "â•æ­£é¢è¯æ¡" æˆ– "â–è´Ÿé¢è¯æ¡" æŒ‰é’®ç›´æ¥æ›¿æ¢å½“å‰å†…å®¹
        - **ğŸ’¡ ä½¿ç”¨æŠ€å·§**ï¼šå…ˆé€‰æ‹©è¯æ¡ï¼Œå†ç‚¹å‡»åº”ç”¨åˆ°å¯¹åº”çš„æç¤ºè¯æ¡†ä¸­
        
        ### ğŸ”„ **ä¼ ç»Ÿå›¾ç”Ÿå›¾ vs ğŸ–¼ï¸ ControlNet è¯¦ç»†å¯¹æ¯”ï¼š**
        
        **ğŸ”¬ ä¼ ç»Ÿå›¾ç”Ÿå›¾æŠ€æœ¯åŸç†ï¼š**
        - ğŸ”§ **VAEç¼–ç å™¨**ï¼šå°†è¾“å…¥å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        - ğŸ² **å™ªå£°è°ƒåˆ¶**ï¼šæ ¹æ®strengthæ·»åŠ ä¸åŒç¨‹åº¦å™ªå£°
        - ğŸ”„ **UNeté‡‡æ ·**ï¼šåœ¨æ½œåœ¨ç©ºé—´ç»“åˆæ–‡æœ¬è¿›è¡Œå»å™ª
        - ğŸ–¼ï¸ **VAEè§£ç å™¨**ï¼šå°†æ½œåœ¨è¡¨ç¤ºè§£ç å›å›¾åƒç©ºé—´
        
        **ä¼ ç»Ÿå›¾ç”Ÿå›¾çš„æŠ€æœ¯é™åˆ¶ï¼š**
        - ğŸ”¸ **APIæ¨¡å¼å—é™**ï¼šéœ€è¦å®Œæ•´VAEç¼–ç å™¨ï¼Œå…¬å…±APIé€šå¸¸ä¸æä¾›
        - ğŸ”¸ **ç»“æ„ä¸ç¨³å®š**ï¼šå™ªå£°è°ƒåˆ¶å¯èƒ½ç ´åé‡è¦ç»“æ„ä¿¡æ¯
        - ğŸ”¸ **å‚æ•°æ•æ„Ÿ**ï¼šstrengthå‚æ•°éš¾ä»¥ç²¾ç¡®æ§åˆ¶å˜åŒ–ç¨‹åº¦
        - ğŸ”¸ **ComfyUIä¼˜åŠ¿**ï¼šé€šè¿‡ç‹¬ç«‹VAEèŠ‚ç‚¹å®ç°ç²¾ç¡®æ§åˆ¶
        
        **ğŸŒ APIæ¨¡å¼ vs ğŸ  æœ¬åœ°æ¨¡å¼ï¼š**
        - **APIæ¨¡å¼**ï¼šâŒ ç¼ºå°‘VAEç¼–ç å™¨è®¿é—®ï¼Œæ— æ³•è¿›è¡Œæ½œåœ¨ç©ºé—´æ“ä½œ
        - **æœ¬åœ°æ¨¡å¼**ï¼šâœ… å®Œæ•´img2imgç®¡é“ï¼ŒåŒ…å«ç‹¬ç«‹VAEç¼–ç å™¨/è§£ç å™¨
        
        **ControlNetçš„æŠ€æœ¯ä¼˜åŠ¿ï¼š**
        - âœ… **ç»“æ„ä¿æŒ**ï¼šé€šè¿‡è¾¹ç¼˜ã€æ·±åº¦ç­‰æ§åˆ¶ä¿¡å·ä¿æŒç»“æ„
        - âœ… **APIå…¼å®¹**ï¼šå¯é€šè¿‡é¢„å¤„ç†å›¾åƒ+æ–‡ç”Ÿå›¾å®ç°
        - âœ… **å¯é¢„æµ‹æ€§**ï¼šç›¸åŒæ§åˆ¶ä¿¡å·äº§ç”Ÿä¸€è‡´ç»“æœ
        - âœ… **é«˜ä¿çœŸåº¦**ï¼šä¿æŒåŸå›¾å…³é”®ç‰¹å¾çš„åŒæ—¶è¿›è¡Œé£æ ¼è½¬æ¢
        
        ### ğŸ› ï¸ **å‚æ•°è°ƒèŠ‚å»ºè®®ï¼š**
        - **é‡‡æ ·æ­¥æ•°**ï¼š20-30 (è´¨é‡ä¸é€Ÿåº¦å¹³è¡¡)
        - **å¼•å¯¼å¼ºåº¦**ï¼š7-12 (æ–‡æœ¬æè¿°å½±å“åŠ›)
        - **å˜åŒ–å¼ºåº¦**(ä¼ ç»Ÿå›¾ç”Ÿå›¾)ï¼š0.6-0.8 (ä¿ç•™åŸå›¾ç¨‹åº¦)
        - **ControlNetå¼ºåº¦**ï¼š0.8-1.2 (ç»“æ„æ§åˆ¶å¼ºåº¦)
        """)
        
        # ç»‘å®šäº‹ä»¶
        
        # API Token è®¾ç½®äº‹ä»¶
        def update_api_token(token):
            global HF_API_TOKEN
            HF_API_TOKEN = token.strip() if token else None
            return f"ğŸ”‘ API Token {'å·²è®¾ç½®' if token else 'æœªè®¾ç½®'}"
        
        # è¿è¡Œæ¨¡å¼åˆ‡æ¢äº‹ä»¶ - æ›´æ–°æ¨¡å‹é€‰æ‹©å™¨å’Œæ˜¾ç¤º
        def update_run_mode_and_models(mode):
            global RUN_MODE
            RUN_MODE = mode
            mode_text = "ğŸŒ APIæ¨¡å¼" if mode == "api" else "ğŸ’» æœ¬åœ°æ¨¡å¼"
            storage_text = "å­˜å‚¨å ç”¨: 0 GB" if mode == "api" else "å­˜å‚¨å ç”¨: 4-10 GB"
            status_text = f"âš™ï¸ {mode_text}\nğŸ’¾ {storage_text}"
            
            # åŒæ—¶æ›´æ–°æ¨¡å‹é€‰æ‹©å™¨
            model_update = update_model_choices(mode)
            return status_text, model_update
        
        run_mode_radio.change(
            update_run_mode_and_models,
            inputs=[run_mode_radio],
            outputs=[current_model_display, model_dropdown]
        )
        
        # ä»£ç†è®¾ç½®äº‹ä»¶
        def update_proxy_settings(enabled, http_proxy, https_proxy):
            status = update_proxy_config(enabled, http_proxy, https_proxy)
            return status
        
        proxy_enabled.change(
            update_proxy_settings,
            inputs=[proxy_enabled, http_proxy_input, https_proxy_input],
            outputs=[proxy_status]
        )
        
        http_proxy_input.change(
            update_proxy_settings,
            inputs=[proxy_enabled, http_proxy_input, https_proxy_input],
            outputs=[proxy_status]
        )
        
        https_proxy_input.change(
            update_proxy_settings,
            inputs=[proxy_enabled, http_proxy_input, https_proxy_input],
            outputs=[proxy_status]
        )
        
        # GitHub æ¨é€äº‹ä»¶
        push_to_github_btn.click(
            auto_push_to_github,
            inputs=[],
            outputs=[github_status]
        )
        
        # API Token å®æ—¶éªŒè¯
        api_token_input.change(
            validate_api_key,
            inputs=[api_token_input],
            outputs=[token_status]
        )
        
        # æ¨¡å‹APIæ”¯æŒæ£€æµ‹
        def update_model_api_status(model_id, run_mode):
            return check_model_api_support(model_id, run_mode)
        
        model_dropdown.change(
            update_model_api_status,
            inputs=[model_dropdown, run_mode_radio],
            outputs=[model_api_status]
        )
        
        run_mode_radio.change(
            update_model_api_status,
            inputs=[model_dropdown, run_mode_radio],
            outputs=[model_api_status]
        )
        
        # APIè¿æ¥æµ‹è¯•
        test_api_btn.click(
            test_model_api_connection,
            inputs=[model_dropdown, api_token_input],
            outputs=[model_api_status]
        )
        
        # ControlNet APIè¿æ¥æµ‹è¯•
        test_controlnet_api_btn.click(
            test_controlnet_api_connection,
            inputs=[controlnet_test_dropdown, api_token_input],
            outputs=[controlnet_api_status]
        )
        
        # æ¨¡å‹åŠ è½½äº‹ä»¶
        load_btn.click(
            load_models, 
            inputs=[run_mode_radio, model_dropdown, controlnet_dropdown, api_token_input], 
            outputs=[load_status]
        )
        
        # æ›´æ–°å½“å‰æ¨¡å‹æ˜¾ç¤º
        model_dropdown.change(
            lambda x: f"ğŸ“¦ é€‰ä¸­æ¨¡å‹: {MODELS.get(x, x)}",
            inputs=[model_dropdown],
            outputs=[current_model_display]
        )
        
        # Prompt è¾…åŠ©å™¨äº‹ä»¶
        def get_selected_positive_tags(*tag_groups):
            """è·å–æ‰€æœ‰é€‰ä¸­çš„æ­£é¢æ ‡ç­¾"""
            selected_tags = []
            for tags in tag_groups:
                if tags:
                    selected_tags.extend(tags)
            return ", ".join(selected_tags) if selected_tags else ""
        
        def get_selected_negative_tags(*tag_groups):
            """è·å–æ‰€æœ‰é€‰ä¸­çš„è´Ÿé¢æ ‡ç­¾"""
            selected_tags = []
            for tags in tag_groups:
                if tags:
                    selected_tags.extend(tags)
            return ", ".join(selected_tags) if selected_tags else ""
        
        def clear_all_tags():
            return [[] for _ in range(14)]  # 7ä¸ªæ­£é¢tagç»„ + 7ä¸ªè´Ÿé¢tagç»„
        
        # æ­£é¢è¯æ¡åº”ç”¨åˆ°å„ä¸ªpromptæ¡†çš„äº‹ä»¶
        apply_positive_to_prompt1.click(
            get_selected_positive_tags,
            inputs=[quality_tags, style_tags, lighting_tags, composition_tags, mood_tags, scene_tags, color_tags],
            outputs=[prompt1]
        )
        
        apply_positive_to_img2img.click(
            get_selected_positive_tags,
            inputs=[quality_tags, style_tags, lighting_tags, composition_tags, mood_tags, scene_tags, color_tags],
            outputs=[prompt_img2img]
        )
        
        apply_positive_to_prompt2.click(
            get_selected_positive_tags,
            inputs=[quality_tags, style_tags, lighting_tags, composition_tags, mood_tags, scene_tags, color_tags],
            outputs=[prompt2]
        )
        
        # è´Ÿé¢è¯æ¡åº”ç”¨åˆ°å„ä¸ªnegative promptæ¡†çš„äº‹ä»¶
        apply_negative_to_prompt1.click(
            get_selected_negative_tags,
            inputs=[neg_quality_tags, neg_anatomy_tags, neg_face_tags, neg_style_tags, neg_tech_tags, neg_lighting_tags, neg_composition_tags],
            outputs=[negative_prompt1]
        )
        
        apply_negative_to_img2img.click(
            get_selected_negative_tags,
            inputs=[neg_quality_tags, neg_anatomy_tags, neg_face_tags, neg_style_tags, neg_tech_tags, neg_lighting_tags, neg_composition_tags],
            outputs=[negative_prompt_img2img]
        )
        
        apply_negative_to_prompt2.click(
            get_selected_negative_tags,
            inputs=[neg_quality_tags, neg_anatomy_tags, neg_face_tags, neg_style_tags, neg_tech_tags, neg_lighting_tags, neg_composition_tags],
            outputs=[negative_prompt2]
        )
        
        # å…¨å±€åº”ç”¨æŒ‰é’®äº‹ä»¶ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰
        apply_positive_tags_btn.click(
            get_selected_positive_tags,
            inputs=[quality_tags, style_tags, lighting_tags, composition_tags, mood_tags, scene_tags, color_tags],
            outputs=[]
        )
        
        apply_negative_tags_btn.click(
            get_selected_negative_tags,
            inputs=[neg_quality_tags, neg_anatomy_tags, neg_face_tags, neg_style_tags, neg_tech_tags, neg_lighting_tags, neg_composition_tags],
            outputs=[]
        )
        
        clear_tags_btn.click(
            clear_all_tags,
            outputs=[quality_tags, style_tags, lighting_tags, composition_tags, mood_tags, scene_tags, color_tags,
                    neg_quality_tags, neg_anatomy_tags, neg_face_tags, neg_style_tags, neg_tech_tags, neg_lighting_tags, neg_composition_tags]
        )
        
        # åŸæœ‰çš„ç”Ÿæˆäº‹ä»¶
        generate_btn1.click(
            generate_image,
            inputs=[prompt1, negative_prompt1, num_steps1, guidance_scale1, width1, height1, seed1],
            outputs=[output_image1, output_status1]
        )
        
        generate_btn_img2img.click(
            generate_img2img,
            inputs=[prompt_img2img, negative_prompt_img2img, input_image, strength, num_steps_img2img, guidance_scale_img2img, width_img2img, height_img2img, seed_img2img],
            outputs=[output_image_img2img, output_status_img2img]
        )
        
        # img2img APIæµ‹è¯•æŒ‰é’®
        test_img2img_api_btn.click(
            test_img2img_api_connection,
            inputs=[api_token_input],
            outputs=[output_status_img2img]
        )
        
        generate_btn2.click(
            generate_controlnet_image,
            inputs=[prompt2, negative_prompt2, control_image, control_type_radio, num_steps2, guidance_scale2, controlnet_scale, width2, height2, seed2],
            outputs=[output_image2, control_preview, output_status2]
        )
        
        # æ›´æ–°æ¨¡å‹é€‰æ‹©å™¨
        run_mode_radio.change(
            update_model_choices,
            inputs=[run_mode_radio],
            outputs=[model_dropdown]
        )
        
        return demo

def query_hf_api(endpoint, payload, api_token=None):
    """Call Hugging Face API with proxy support"""
    headers = {"Content-Type": "application/json"}
    if api_token:
        headers["Authorization"] = f"Bearer {api_token}"
    
    # é…ç½®ä»£ç†
    proxies = {}
    if PROXY_CONFIG["enabled"]:
        if PROXY_CONFIG["http"]:
            proxies["http"] = PROXY_CONFIG["http"]
        if PROXY_CONFIG["https"]:
            proxies["https"] = PROXY_CONFIG["https"]
    
    try:
        # å¢åŠ è¶…æ—¶æ—¶é—´å¹¶ä½¿ç”¨ä»£ç†
        response = requests.post(
            endpoint, 
            headers=headers, 
            json=payload, 
            timeout=120,  # å¢åŠ åˆ°2åˆ†é’Ÿ
            proxies=proxies if proxies else None
        )
        
        if response.status_code == 200:
            return response.content
        elif response.status_code == 503:
            raise Exception("Model is loading, please try again later")
        elif response.status_code == 429:
            raise Exception("API rate limit exceeded, please try again later")
        elif response.status_code == 401:
            raise Exception("Invalid or missing API token")
        elif response.status_code == 404:
            raise Exception("Model endpoint not found")
        else:
            # Ensure error message is ASCII safe
            error_text = "Unknown API error"
            try:
                if response.text:
                    # Try to get ASCII-safe error message
                    error_text = response.text.encode('ascii', 'ignore').decode('ascii')
                    if not error_text.strip():
                        error_text = "API error with non-ASCII response"
            except:
                error_text = "API response encoding error"
            raise Exception(f"API call failed: {response.status_code}, {error_text}")
    except requests.exceptions.Timeout:
        proxy_info = f" (using proxy: {proxies})" if proxies else " (no proxy)"
        raise Exception(f"API call timeout after 120s{proxy_info}, please check network connection or proxy settings")
    except requests.exceptions.ConnectionError as e:
        proxy_info = f" (using proxy: {proxies})" if proxies else " (no proxy)"
        raise Exception(f"Network connection error{proxy_info}, please check network settings or try enabling proxy")
    except Exception as e:
        # Ensure all error messages are ASCII safe
        error_msg = str(e)
        try:
            error_msg.encode('ascii')
        except UnicodeEncodeError:
            error_msg = "API call error with encoding issues"
        raise Exception(error_msg)

def generate_image_api(prompt, negative_prompt="", model_id="runwayml/stable-diffusion-v1-5"):
    """Generate image using API"""
    endpoint = API_ENDPOINTS.get(model_id)
    if not endpoint:
        raise Exception(f"Model {model_id} does not support API mode")
    
    # Ensure prompt and negative_prompt are ASCII safe
    try:
        safe_prompt = prompt.encode('utf-8', 'ignore').decode('utf-8')
        safe_negative_prompt = negative_prompt.encode('utf-8', 'ignore').decode('utf-8') if negative_prompt else ""
    except:
        safe_prompt = "safe prompt"
        safe_negative_prompt = ""
    
    payload = {
        "inputs": safe_prompt,
        "parameters": {
            "negative_prompt": safe_negative_prompt,
            "num_inference_steps": 20,
            "guidance_scale": 7.5,
        }
    }
    
    try:
        image_bytes = query_hf_api(endpoint, payload, HF_API_TOKEN)
        image = Image.open(io.BytesIO(image_bytes))
        return image, "API image generation successful!"
    except Exception as e:
        return None, f"API generation failed: {str(e)}"

def generate_controlnet_image_api(prompt, negative_prompt, control_image, control_type):
    """Generate ControlNet image using API - ä¿®å¤ç‰ˆæœ¬"""
    endpoint = CONTROLNET_API_ENDPOINTS.get(control_type)
    if not endpoint:
        raise Exception(f"ControlNet type {control_type} does not support API mode")
    
    # æ£€æŸ¥API Token
    if not HF_API_TOKEN or not HF_API_TOKEN.strip():
        raise Exception("ControlNet API requires a valid Hugging Face API Token. Please set your token in the API settings.")
    
    # Convert control image to base64
    import base64
    import io
    
    buffered = io.BytesIO()
    control_image.save(buffered, format="PNG")
    control_image_b64 = base64.b64encode(buffered.getvalue()).decode()
    
    # Ensure prompt and negative_prompt are safe
    try:
        safe_prompt = prompt.encode('utf-8', 'ignore').decode('utf-8')
        safe_negative_prompt = negative_prompt.encode('utf-8', 'ignore').decode('utf-8') if negative_prompt else ""
    except:
        safe_prompt = "safe prompt"
        safe_negative_prompt = ""
    
    # ä½¿ç”¨ç»è¿‡æµ‹è¯•éªŒè¯çš„ ControlNet API æ ¼å¼
    # æ ¹æ®æµ‹è¯•ç»“æœï¼Œä½¿ç”¨ç®€å•çš„inputsæ ¼å¼
    payload = {
        "inputs": safe_prompt,
        "parameters": {
            "image": control_image_b64,
            "negative_prompt": safe_negative_prompt,
            "num_inference_steps": 20,
            "guidance_scale": 7.5
        }
    }
    
    try:
        image_bytes = query_hf_api(endpoint, payload, HF_API_TOKEN)
        image = Image.open(io.BytesIO(image_bytes))
        control_type_name = CONTROLNET_TYPES[control_type]['name']
        return image, f"âœ… APIæ¨¡å¼ {control_type_name} å›¾åƒç”ŸæˆæˆåŠŸï¼"
    except Exception as e:
        error_msg = str(e)
        if "Model endpoint not found" in error_msg or "404" in error_msg:
            return None, f"âŒ ControlNetæ¨¡å‹ {control_type} ç«¯ç‚¹ä¸å¯ç”¨ã€‚å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) å°è¯•å…¶ä»–æ§åˆ¶ç±»å‹ 3) ä½¿ç”¨æœ¬åœ°æ¨¡å¼"
        elif "401" in error_msg or "Invalid" in error_msg or "credentials" in error_msg.lower():
            return None, f"âŒ API Tokenæ— æ•ˆæˆ–æœªè®¾ç½®ã€‚è¯·åœ¨APIè®¾ç½®ä¸­è¾“å…¥æœ‰æ•ˆçš„ Hugging Face Token"
        elif "503" in error_msg or "loading" in error_msg.lower():
            return None, f"â³ ControlNetæ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œè¯·ç¨ç­‰1-2åˆ†é’Ÿåé‡è¯•"
        elif "timeout" in error_msg.lower():
            return None, f"â° è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¯ç”¨ä»£ç†è®¾ç½®"
        else:
            return None, f"âŒ ControlNet APIè°ƒç”¨å¤±è´¥: {error_msg}"

def generate_img2img_api(prompt, negative_prompt, input_image, strength):
    """Generate img2img image using API - æŠ€æœ¯è¯´æ˜ç‰ˆæœ¬"""
    
    # æŠ€æœ¯åŸç†è¯´æ˜ï¼šimg2imgéœ€è¦å®Œæ•´çš„VAEç¼–ç /è§£ç æµç¨‹
    # 1. VAEç¼–ç å™¨å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
    # 2. æ ¹æ®strengthæ·»åŠ å™ªå£°
    # 3. UNetè¿›è¡Œå»å™ªé‡‡æ ·
    # 4. VAEè§£ç å™¨è§£ç å›å›¾åƒ
    # è¿™ä¸ªå¤æ‚æµç¨‹ä¸é€‚åˆç®€åŒ–çš„APIè°ƒç”¨
    
    
    # æ£€æŸ¥API Token
    if not HF_API_TOKEN or not HF_API_TOKEN.strip():
        return None, "âŒ img2img APIéœ€è¦æœ‰æ•ˆçš„ Hugging Face API Token"
    
    # æŠ€æœ¯é™åˆ¶è¯´æ˜
    return None, f"""âŒ img2img APIåŠŸèƒ½å—é™

ğŸ”¬ æŠ€æœ¯åŸç†è¯´æ˜:
img2imgéœ€è¦å®Œæ•´çš„VAEç¼–ç /è§£ç æµç¨‹ï¼š
1. VAEç¼–ç å™¨ï¼šå°†è¾“å…¥å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
2. å™ªå£°è°ƒåˆ¶ï¼šæ ¹æ®strengthå‚æ•°æ·»åŠ å™ªå£°
3. UNeté‡‡æ ·ï¼šåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œå»å™ª
4. VAEè§£ç å™¨ï¼šå°†ç»“æœè§£ç å›å›¾åƒç©ºé—´

ğŸš« APIé™åˆ¶:
â€¢ å…¬å…±APIé€šå¸¸åªæä¾›ç®€åŒ–çš„text-to-imageç«¯ç‚¹
â€¢ img2imgéœ€è¦å®Œæ•´çš„æ¨¡å‹ç®¡é“ï¼ˆVAE+UNet+è°ƒåº¦å™¨ï¼‰
â€¢ å¤æ‚çš„æ½œåœ¨ç©ºé—´æ“ä½œä¸é€‚åˆAPIå°è£…

ğŸ’¡ æ¨èè§£å†³æ–¹æ¡ˆ:
1. ğŸ  æœ¬åœ°æ¨¡å¼ - å®Œæ•´æ”¯æŒimg2imgæµç¨‹
2. ğŸ–¼ï¸ ControlNetæ¨¡å¼ - å¯å®ç°ç±»ä¼¼çš„å›¾åƒå¼•å¯¼æ•ˆæœï¼š
   â€¢ Cannyè¾¹ç¼˜æ£€æµ‹ + æ–‡ç”Ÿå›¾
   â€¢ ä¿æŒåŸå›¾ç»“æ„ï¼Œæ”¹å˜é£æ ¼
   â€¢ APIæ¨¡å¼å®Œå…¨æ”¯æŒ
3. ï¿½ Inpaintingæ¨¡å¼ - å±€éƒ¨ä¿®æ”¹ï¼ˆå¦‚æœæ”¯æŒï¼‰

ğŸ¯ ComfyUIå¯¹æ¯”:
æ‚¨æåˆ°çš„ComfyUIç¡®å®æ˜¯å®Œæ•´çš„æœ¬åœ°å·¥ä½œæµï¼Œ
åŒ…å«å®Œæ•´çš„VAEç¼–ç å™¨ï¼Œè¿™æ­£æ˜¯APIæ¨¡å¼ç¼ºå°‘çš„éƒ¨åˆ†ã€‚"""

def update_model_choices(run_mode):
    """æ ¹æ®è¿è¡Œæ¨¡å¼åŠ¨æ€æ›´æ–°æ¨¡å‹é€‰æ‹©å™¨"""
    available_models = get_available_models(run_mode)
    
    if run_mode == "api":
        # APIæ¨¡å¼ï¼šåªæ˜¾ç¤ºæ”¯æŒAPIçš„æ¨¡å‹ï¼ŒæŒ‰æ¨èç¨‹åº¦æ’åº
        recommended_order = [
            "black-forest-labs/FLUX.1-dev",
            "black-forest-labs/FLUX.1-schnell", 
            "stabilityai/stable-diffusion-xl-base-1.0",
            "stabilityai/stable-diffusion-3.5-large",
            "stabilityai/stable-diffusion-3-medium-diffusers",
            "latent-consistency/lcm-lora-sdxl",
            "Kwai-Kolors/Kolors",
            "runwayml/stable-diffusion-v1-5",
            "stabilityai/stable-diffusion-2-1",
            "prompthero/openjourney",
            "dreamlike-art/dreamlike-diffusion-1.0"
        ]
        
        choices = []
        for model_id in recommended_order:
            if model_id in available_models:
                model_name = available_models[model_id]
                choices.append((model_name, model_id))
        
        # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨èæ¨¡å‹
        default_value = recommended_order[0] if recommended_order else "black-forest-labs/FLUX.1-dev"
        
        return gr.Dropdown.update(
            choices=choices,
            value=default_value,
            label="ğŸ¤– é€‰æ‹©åŸºç¡€æ¨¡å‹ (ä»…APIæ”¯æŒçš„æ¨¡å‹)",
            info="âœ… APIæ¨¡å¼ - è¿™äº›æ¨¡å‹æ”¯æŒäº‘ç«¯æ¨ç†ï¼Œæ— éœ€ä¸‹è½½"
        )
    else:
        # æœ¬åœ°æ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹
        choices = [(name, model_id) for model_id, name in available_models.items()]
        return gr.Dropdown.update(
            choices=choices,
            value="runwayml/stable-diffusion-v1-5",
            label="ğŸ¤– é€‰æ‹©åŸºç¡€æ¨¡å‹ (æ”¯æŒæ‰€æœ‰æ¨¡å‹)",
            info="ğŸ’¾ æœ¬åœ°æ¨¡å¼ - é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆ4-10GBï¼‰"
        )

# ä¸»å‡½æ•°ï¼šå¯åŠ¨Gradioåº”ç”¨ - å¸¦è‡ªåŠ¨æ¸…ç†åŠŸèƒ½
if __name__ == "__main__":
    print("ğŸ¨ å¯åŠ¨ AI å›¾åƒç”Ÿæˆå™¨...")
    print("=" * 60)
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç•Œé¢...")
    
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    gradio_app = demo  # ä¿å­˜åˆ°å…¨å±€å˜é‡
    
    print("âœ… ç•Œé¢åˆå§‹åŒ–å®Œæˆï¼")
    print("ğŸŒ æ­£åœ¨å¯åŠ¨æœåŠ¡å™¨...")
    print("=" * 60)
    
    # æ™ºèƒ½ç«¯å£åˆ†é…
    ports_to_try = [7860, 7861, 7862, 7863, 7864]
    
    for port in ports_to_try:
        try:
            print(f"ğŸ”„ å°è¯•å¯åŠ¨åœ¨ç«¯å£ {port}...")
            current_port = port  # ä¿å­˜å½“å‰ç«¯å£åˆ°å…¨å±€å˜é‡
            
            demo.launch(
                server_name="0.0.0.0",
                server_port=port,
                share=False,
                inbrowser=True,
                show_error=True,
                debug=False
            )
            break
            
        except (OSError, Exception) as e:
            if "Address already in use" in str(e) or "10048" in str(e) or "Cannot find empty port" in str(e):
                print(f"âš ï¸ ç«¯å£ {port} è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£...")
                continue
            else:
                print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
                cleanup_resources()
                sys.exit(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­å¯åŠ¨")
            cleanup_resources()
            sys.exit(0)
    
    # ç¨‹åºç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†
    cleanup_resources()